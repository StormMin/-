{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝\n",
    "기계가 분석가가 의도한 대로, 데이터로부터 학습하여 문제를 해결하는 기술. 엔지니어링 과정.\n",
    "\n",
    "## 0. 머신러닝의 문제들\n",
    "1. 회귀(Regression in Supervised Learning)\n",
    " - target y의 정확한 value를 예측하는 문제\n",
    " - target이 여러개가 있을 수도 있고, 한 개만 있을 수도 있다.\n",
    " - 데이터 X, 타겟 y가 존재\n",
    " - X.shape = (n, p), y.shape = (n,)\n",
    "2. 분류(Classification in Supervised Learning)\n",
    " - target y이 어떤 범주값을 가지고 있고, 데이터를 y의 범주값으로 매칭. 분류\n",
    " - 범주가 여러개가 있을 수 있고, 2개만 있을 수 있다.\n",
    " - binary problem으로 모두 해결할 수 있다.\n",
    " - 데이터 X, 타겟 y가 존재\n",
    " - X.shape = (n, p), y.shape = (n,)\n",
    "3. 비지도학습(Unsupervised Learning)\n",
    " - 지도학습에 비해 훨씬 어렵다. 답이 없다.\n",
    " - 분석 한 이후, 설득하기 위한 보고서 작업, 시각화이 매우 중요\n",
    " - 데이터 내에 있는 미지의 패턴을 식별하는 작업\n",
    " - 지도학습의 입력으로도 사용된다.\n",
    "4. 강화학습(Reinforcement Learning)\n",
    " - 답은 모르지만, 바람직한 방향은 알고 있는 문제\n",
    " - trial error로부터 개선방향을 찾는 문제\n",
    " - env -> state(s(t)) -> policy(pi) -> action(a) -> env -> state(s(t+1))\n",
    " \n",
    "### 학습 데이터\n",
    "X, y\n",
    "- X: 엑셀의 표, 데이터베이스의 테이블, pandas의 DataFrame\n",
    "- y: 1차원 벡터, 열, pandas의 Series\n",
    "- 행: row, instance, sample, record, tuple, observative, object\n",
    "- 열: column, feature, vector, sequence, attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. linear regression\n",
    "\n",
    "Set : $y = X \\cdot w + b + \\epsilon = X \\cdot w + \\epsilon$\n",
    "- assume for 1 observative : x = (1, x), w = (b, w)\n",
    "\n",
    "We know:  (here, v and X is constant about w)\n",
    "\n",
    "$$\\begin{align}\n",
    "\\cfrac {\\partial}{\\partial w} \\left( v^T \\cdot w \\right) &= v \\\\\n",
    "\\cfrac {\\partial}{\\partial w} \\left( w^T \\cdot v \\right) &= v \\\\\n",
    "(w^T \\cdot A)^T &= A^T \\cdot w \\\\\n",
    "\\cfrac {\\partial}{\\partial w} \\left( w^T \\cdot A \\cdot w \\right) &= A \\cdot w + A^T \\cdot w\n",
    "\\end{align}$$\n",
    "\n",
    "So, We solve: y.shape = (N, ), X.shape=(N, p+1), w.shape = (p+1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "y &= X \\cdot w + \\epsilon = \\hat y + \\epsilon\\\\\n",
    "Loss(w) &= \\epsilon ^2 =||y - \\hat y||^2 = (y - X \\cdot w)^T \\cdot (y - X \\cdot w)\\\\\n",
    "\\cfrac {\\partial Loss}{\\partial w} &= 2 (X^T \\cdot X) \\cdot w - 2X^T \\cdot y = 0 \n",
    "\\end{align}$$\n",
    "\n",
    "We get $\\hat w$ :\n",
    "$$ \\left| {\\cfrac {\\partial Loss}{\\partial w}} \\right|_{w = \\hat w} = 0 \\Rightarrow \n",
    "\\hat w = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y $$\n",
    "\n",
    "### 1.1 Using statistical tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "bunch = load_iris()\n",
    "bunch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunch.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.DataFrame(bunch.data, columns=bunch.feature_names)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   sl      150 non-null    float64\n",
      " 1   sw      150 non-null    float64\n",
      " 2   pl      150 non-null    float64\n",
      " 3   pw      150 non-null    float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 4.8 KB\n"
     ]
    }
   ],
   "source": [
    "iris.columns = ['sl', 'sw', 'pl', 'pw']\n",
    "iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "petal width를 target으로 회귀식을 생성해보자. 통계에서 회귀식의 formula는 아래와 같이 표현한다.\n",
    "$$y \\sim x_1 + x_2 + x_3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sl + sw + pl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' + '.join(iris.columns[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pw ~ sl + sw + pl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# formula = 'y ~ x1 + x2 + x3'\n",
    "features = iris.columns\n",
    "formula = '%s ~ '%iris.columns[3]\n",
    "formula += ' + '.join(iris.columns[:3])\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>pw</td>        <th>  R-squared:         </th> <td>   0.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   734.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 22 Jul 2021</td> <th>  Prob (F-statistic):</th> <td>7.83e-88</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:36:04</td>     <th>  Log-Likelihood:    </th> <td>  36.751</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   150</td>      <th>  AIC:               </th> <td>  -65.50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   146</td>      <th>  BIC:               </th> <td>  -53.46</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.2403</td> <td>    0.178</td> <td>   -1.347</td> <td> 0.180</td> <td>   -0.593</td> <td>    0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sl</th>        <td>   -0.2073</td> <td>    0.048</td> <td>   -4.363</td> <td> 0.000</td> <td>   -0.301</td> <td>   -0.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sw</th>        <td>    0.2228</td> <td>    0.049</td> <td>    4.553</td> <td> 0.000</td> <td>    0.126</td> <td>    0.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pl</th>        <td>    0.5241</td> <td>    0.024</td> <td>   21.399</td> <td> 0.000</td> <td>    0.476</td> <td>    0.572</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.609</td> <th>  Durbin-Watson:     </th> <td>   1.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.061</td> <th>  Jarque-Bera (JB):  </th> <td>   6.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.223</td> <th>  Prob(JB):          </th> <td>  0.0332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.944</td> <th>  Cond. No.          </th> <td>    90.1</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     pw   R-squared:                       0.938\n",
       "Model:                            OLS   Adj. R-squared:                  0.937\n",
       "Method:                 Least Squares   F-statistic:                     734.4\n",
       "Date:                Thu, 22 Jul 2021   Prob (F-statistic):           7.83e-88\n",
       "Time:                        13:36:04   Log-Likelihood:                 36.751\n",
       "No. Observations:                 150   AIC:                            -65.50\n",
       "Df Residuals:                     146   BIC:                            -53.46\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.2403      0.178     -1.347      0.180      -0.593       0.112\n",
       "sl            -0.2073      0.048     -4.363      0.000      -0.301      -0.113\n",
       "sw             0.2228      0.049      4.553      0.000       0.126       0.320\n",
       "pl             0.5241      0.024     21.399      0.000       0.476       0.572\n",
       "==============================================================================\n",
       "Omnibus:                        5.609   Durbin-Watson:                   1.573\n",
       "Prob(Omnibus):                  0.061   Jarque-Bera (JB):                6.811\n",
       "Skew:                           0.223   Prob(JB):                       0.0332\n",
       "Kurtosis:                       3.944   Cond. No.                         90.1\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model = smf.ols(formula = formula, data = iris)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept   -0.240307\n",
       "sl          -0.207266\n",
       "sw           0.222829\n",
       "pl           0.524083\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using numpy with linear algebra\n",
    "Now, we'll compute with numpy :\n",
    "$$ \\hat w =  (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set : $y = X \\cdot w + b + \\epsilon = X \\cdot w + \\epsilon$\n",
    "- assume: x = (1, x), w = (b, w). So,\n",
    "- y = X[:, 3]\n",
    "- X = np.hstack(np.ones(shape), X[:, :3])\n",
    "\n",
    "And then, you can use numpy.linalg.inv for inverse matrix.\n",
    "\n",
    "[Quiz] numpy를 이용하여 $\\hat w$를 계산하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 5.1, 3.5, 1.4],\n",
       "       [1. , 4.9, 3. , 1.4],\n",
       "       [1. , 4.7, 3.2, 1.3],\n",
       "       [1. , 4.6, 3.1, 1.5],\n",
       "       [1. , 5. , 3.6, 1.4]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = bunch.data[:, 3]\n",
    "X = bunch.data[:, :3]\n",
    "X = np.hstack((np.ones(X.shape[0]).reshape(-1, 1), X))\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24030739, -0.20726607,  0.22282854,  0.52408311])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.linalg import inv\n",
    "w = inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept   -0.240307\n",
       "sl          -0.207266\n",
       "sw           0.222829\n",
       "pl           0.524083\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat y = X \\cdot w$로 예측값을 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.76472775, 1.63140328, 1.80618096, 2.06230882, 1.87813229])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dot(w)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.3, 1.9, 2. , 2.3, 1.8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀모델에서 계산한 $R^2$의 정체\n",
    "$$\\begin{align} \n",
    "R^2 &= \\text{cor}^2(y, \\hat y)\\\\\n",
    "\\text{cor}(y, \\hat y) &= \\cfrac {(y - \\bar y)\\cdot(\\hat y - \\bar {\\hat y})}{n \\sigma_y \\sigma_{\\hat y}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9378502736046809"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9378502736046815"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = X.dot(w)\n",
    "np.square(np.mean((y - y.mean())*(yhat - yhat.mean()))/(y.std()*yhat.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9378502736046817"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).corr(pd.Series(yhat))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Machine Learning Method\n",
    "![](https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F994F59375D818E5E36E75D)\n",
    "$$\\text{SGD(경사하강법); Stochastic Gradient Descent}$$\n",
    "\n",
    "1. 전체 학습데이터의 일부분(batch)을 이용하여 예측을 수행하고\n",
    "2. 실제 값에서 예측값을 빼서 error구해 손실을 계산하고\n",
    "3. 손실을 줄이는 방향으로 학습 파라미터(w)를 업데이트하고\n",
    "4. 1 ~ 3을 전체 학습데이터를 모두 이용할 때까지 반복한 후,\n",
    "5. 학습데이터를 다시 섞고 1 ~ 4의 과정을 EPOCH의 수만큼 반복한다.\n",
    "\n",
    "$$\\begin{align}\n",
    "Loss(w) &= \\epsilon^2 = (y - X \\cdot w)^T \\cdot (y - X \\cdot w)\\\\\n",
    "dW = \\cfrac {\\partial Loss}{\\partial w} &= 2 (X^T \\cdot X) \\cdot w - 2X^T \\cdot y \\\\\n",
    "w(t+1) &= w(t) - \\text{lr} * \\cfrac {\\partial Loss}{\\partial w} \\Bigg|_{w =w(t)}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46903266, -0.24571793,  0.41240044,  0.4701742 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(234)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20000 # 50000\n",
    "batch = 16\n",
    "lr = 0.0001 # 0.00005\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "        y_hat = x_batch.dot(w)\n",
    "        loss += (y_batch - y_hat).dot((y_batch - y_hat))\n",
    "        dw = X.T.dot(X).dot(w) - X.T.dot(y) # = d Loss(w) / d(w)\n",
    "        w -= lr*dw\n",
    "        \n",
    "    loss /= rows\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "Loss(w) &= (y - X \\cdot w)^T \\cdot (y - X \\cdot w)\\\\\n",
    "\\cfrac {\\partial Loss}{\\partial w} &= 2 (X^T \\cdot X) \\cdot w - 2X^T \\cdot y \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24030739, -0.20726607,  0.22282854,  0.52408311])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24030739, -0.20726607,  0.22282854,  0.52408311])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "loss_df = pd.DataFrame(losses)\n",
    "loss_df[:100].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df[19000:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Quiz] 다음과 같은 그래프를 그린다:\n",
    "- y, yhat 간의 산점도를 그린다.\n",
    "- error, yhat 간의 산점도를 그니다.\n",
    "- y, yhat을 모두 line 그래프로 그린다.\n",
    "- y, yhat, error에 대한 histogram을 그린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.216252</td>\n",
       "      <td>-0.016252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.146291</td>\n",
       "      <td>0.053709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.179901</td>\n",
       "      <td>0.020099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.283162</td>\n",
       "      <td>-0.083162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.259261</td>\n",
       "      <td>-0.059261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target      pred     error\n",
       "0     0.2  0.216252 -0.016252\n",
       "1     0.2  0.146291  0.053709\n",
       "2     0.2  0.179901  0.020099\n",
       "3     0.2  0.283162 -0.083162\n",
       "4     0.2  0.259261 -0.059261"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = X.dot(w)\n",
    "regDf = pd.DataFrame({'target':y, 'pred':yhat, 'error':y-yhat})\n",
    "regDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2,2, figsize=(12,12))\n",
    "axes = axes.ravel()\n",
    "regDf.plot('pred', 'target', kind='scatter', ax=axes[0])\n",
    "regDf.plot('pred', 'error', kind='scatter', ax=axes[1])\n",
    "idx = regDf.target.argsort().values\n",
    "regDf.iloc[idx].plot('target', 'pred', kind='line', ax=axes[2])\n",
    "# regDf.plot('target', 'pred', kind='line', ax=axes[2])\n",
    "regDf.plot(kind='hist', ax=axes[3], bins=30, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'target > 0.7')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAF1CAYAAAATPtcEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAvA0lEQVR4nO3df5QcdZno//dDyBqFIBACIhCCuxDAACGOGC7o8vsCIvgDFFYk7EWDRs9Zue5e0d2veNRd3LsIu3slYlxYQBEREURF14C4CAKSxAEDQSAYIJEfAeRHQJCE5/tHV3AYZqYqSXdX9/T7dU6fqR+fqnqqe/LMk09/qioyE0mSJEnD26DuACRJkqROZ9EsSZIklbBoliRJkkpYNEuSJEklLJolSZKkEhbNkiRJUgmLZkmSJKmERbPaLiKWRsRBo/H4EfFXEXFfRDwTEVdExObDtHtrRKwc9MqIeE8r4pKk9WHeNm/LolldKCLG1HjssSMk1DcCXwU+AGwFPAvMGaptZv48Mzde8wKOAFYCP25N5JJUn1bn7Yh4VUS8dh22M2+rMotmtVVEfB2YBHy/+F/6/ymWXxoRD0XEkxFxXZHI1mxzfkR8JSKuiohngP0jYnpE/Coini62vSQivjBgmyMioj8inoiIX0TE7iMdv0LcUyPiS8Ay4OBhmr0f+H5mXpeZK4H/D3h3RIyvcIiZwHcy85kq8UhSu3RJ3t4CeCAiLoqIgyKian1j3lZlFs1qq8z8AHA/8I7if+v/t1j1I2BHYEtgIXDRoE3/CvhHYDzwS+By4Hxgc+Bi4F1rGkbEnsB5wMnABBq9CFdGxKtGOP4rRMRmETE7Im4BfgK8CByQmZcMs8kbgVsHnOsS4I/ATiO9JxGxEXA0cMFI7SSpDt2QtzNzOY1c+yvgLOC3EfG5iHhDyemZt1WZRbM6Qmael5lPZ+bzwGeBPQZ91fa9zLwhM18EpgEbAv+emS9k5ndpJOQ1ZgFfzcybM3N1Zl4APA/MqBJLRGwSEd8CfgvsB5wGbJeZf5eZt4+w6cbAk4OWPUnjD8ZI3g08Cvx3lfgkqRN0Ut4u4nkoM8/IzN1o5NVNgZsi4mcRsccwm5m3VZlFs2oXEWMi4osRsSQingKWFqu2GNDsgQHTrweWZ2YOs3574BPFV3xPRMQTwHbFdlWMBaYCjwP9wKLMXF1hu5XAJoOWbQI8XbLdTODCQecjSR2rA/P2YHfT6EG+B9iZRgE9FPO2KrNoVh0GJ5m/Ao4CDgJeC0wulscw2zwIbBMRA9dvN2D6AeAfM3PTAa/XZObFwxz/5cFlPpaZU4H3AdsCCyPipxFxYkRsPMKmtwMv9WYUXwu+CrhruA0iYjsavdkXjhSTJNWso/M2vFTIHxYRF9MYzvF24HRg28wcrkfYvK3KLJpVh4eBgePMxtP4Gu4x4DXAP5VsfyOwGvhYRGwYEUcBew1Y/zXgwxHxlmjYKCLePuDCjsHHH1Jm3pKZs4FtaIyvex/wu4g4dJhNLgLeEY3bEm0EfA74bmaO1GPxAeAXxTg6SepUHZ23I2JLGhdq/xNwE/AXmfnuzPx+Zq4aIS7ztiqzaFYdTgf+ofgK7m9p/G/9PmA5cAeNhDeszPwjjfFkJwFPAMcDP6CRwMnM+cCHgC8Dv6fx9dyJIxx/RJn5fGZekpmH0fia7zfDtLsd+DCNJPwIjT8qs9esj4gfRcSnB212Al5IIqnzdXrefhY4NDP3zMx/y8xHq5yUeVtrIxyOo9EgIm4GzsnM/6w7FklSOfO2uo09zepKEfGXEfG64mu+mcDueIN5SepY5m11uw3rDkBaR1OAbwMbAfcCR2fmg/WGJEkagXlbXc3hGZIkSVIJh2dIkiRJJSyaJUmSpBJdMaZ5iy22yMmTJ9cdhiSttQULFjyamRPrjqOdzNmSutlwebtlRXNEjAOuo/FknQ2B72TmaRGxA/AtYAKwAPhAcf/GYU2ePJn58+e3KlRJapmIuK/uGNrNnC2pmw2Xt1s5PON54IDM3AOYBhwaETOAfwbOysy/oHED85NaGIMkSZK03lpWNGfDymJ2bPFK4ADgO8XyC4B3tioGSZIkqRlaeiFgRIyJiH4aj6acBywBnhjwHPhlwDbDbDsrIuZHxPwVK1a0MkxJkiRpRC29EDAzVwPTImJT4HJg57XYdi4wF6Cvr8+bSUsd7IUXXmDZsmU899xzdYdSm3HjxrHtttsyduzYukORpBGZsxvWNm+35e4ZmflERFwL7A1sGhEbFr3N2wLL2xGDpNZZtmwZ48ePZ/LkyURE3eG0XWby2GOPsWzZMnbYYYe6w5GkEfV6zoZ1y9stG54REROLHmYi4tXAwcBi4Frg6KLZTOB7rYpBUns899xzTJgwoWeTb0QwYcKEnu+1kdQdej1nw7rl7Vb2NG8NXBARY2gU59/OzB9ExB3AtyLiC8CvgHNbGIOkNunl5Auev6TuYs5a+/eglXfPuC0z98zM3TNzamZ+rlh+b2bulZl/kZnHZObzrYpBUm944oknmDNnTsuPc8UVV3DHHXe0/DiSNJp1a87uiicCSuouZ827q6n7O+XgnUZcvyYBz549u9L+MpPMZIMN1q7f4IorruCII45g1113XavtJKmTmbOraekt5ySpHU499VSWLFnCtGnTOOWUUzjwwAOZPn06u+22G9/7XuOyiaVLlzJlyhROOOEEpk6dygMPPMDnP/95pkyZwr777stxxx3HGWecAcCSJUs49NBDedOb3sRb3/pW7rzzTn7xi19w5ZVX8nd/93dMmzaNJUuW1HnKktS1ujVn29Msqet98YtfZNGiRfT397Nq1SqeffZZNtlkEx599FFmzJjBkUceCcDdd9/NBRdcwIwZM7jlllu47LLLuPXWW3nhhReYPn06b3rTmwCYNWsW55xzDjvuuCM333wzs2fP5qc//SlHHnkkRxxxBEcfffRI4UiSRtCtOduiWdKokpl8+tOf5rrrrmODDTZg+fLlPPzwwwBsv/32zJgxA4AbbriBo446inHjxjFu3Dje8Y53ALBy5Up+8YtfcMwxx7y0z+ef99ILSWqFbsrZFs2SRpWLLrqIFStWsGDBAsaOHcvkyZNfuqXQRhttVLr9iy++yKabbkp/f3+LI5UkdVPOtmiWKqh6kUTZxQ9qjfHjx/P0008D8OSTT7LlllsyduxYrr32Wu67774ht9lnn304+eST+dSnPsWqVav4wQ9+wKxZs9hkk03YYYcduPTSSznmmGPITG677Tb22GOPlx1HWhtz+qvdKWD2tGoXRkndrFtzthcCSup6EyZMYJ999mHq1Kn09/czf/58dtttNy688EJ23nnnIbd585vfzJFHHsnuu+/OYYcdxm677cZrX/taoNHzce6557LHHnvwxje+8aULU4499lj+5V/+hT333NMLASVpHXVrzo7MXO+dtFpfX1/Onz+/7jDUw+xpHtnixYvZZZdd6g5jra1cuZKNN96YZ599lre97W3MnTuX6dOnr/P+hnofImJBZvatb6zdxJz9SvY0q5OYs/9kbfK2wzMk9axZs2Zxxx138NxzzzFz5sz1Tr6SpNapO2dbNEvqWd/85jfrDkGSVFHdOdsxzZIkSVIJi2ZJkiSphEWzJEmSVMKiWZIkSSph0SxJg/zsZz/jiCOOqDsMSVIF7crZ3j1DUvNde3pz97f/p5qym9WrVzNmzJim7EuSRg1zdiX2NEsaFZYuXcrOO+/M+9//fnbZZReOPvponn32WSZPnswnP/lJpk+fzqWXXspPfvIT9t57b6ZPn84xxxzDypUrAfjxj3/MzjvvzPTp0/nud79b89lI0ujWjTnbolnSqPGb3/yG2bNns3jxYjbZZBPmzGk8hW3ChAksXLiQgw46iC984QtcffXVLFy4kL6+Ps4880yee+45PvShD/H973+fBQsW8NBDD9V8Js0VEdtFxLURcUdE3B4Rf1Ms3zwi5kXE3cXPzYbZfmbR5u6ImNne6CWNVt2Wsy2aJY0a2223Hfvssw8Axx9/PNdffz0A73vf+wC46aabuOOOO9hnn32YNm0aF1xwAffddx933nknO+ywAzvuuCMRwfHHH1/bObTIKuATmbkrMAP4aETsCpwKXJOZOwLXFPMvExGbA6cBbwH2Ak4brriWpLXRbTnbMc2SRo2IGHJ+o402AiAzOfjgg7n44otf1q6/v78t8dUlMx8EHiymn46IxcA2wFHAfkWzC4CfAZ8ctPn/BOZl5uMAETEPOBS4GElaD92Ws+1pljRq3H///dx4441A43Gr++6778vWz5gxgxtuuIF77rkHgGeeeYa77rqLnXfemaVLl7JkyRKAVyTo0SQiJgN7AjcDWxUFNcBDwFZDbLIN8MCA+WXFssH7nRUR8yNi/ooVK5obtKRRqdtytkWzpFFjypQpnH322eyyyy78/ve/5yMf+cjL1k+cOJHzzz+f4447jt133529996bO++8k3HjxjF37lze/va3M336dLbccsuazqC1ImJj4DLg45n51MB1mZlAruu+M3NuZvZlZt/EiRPXM1JJvaDbcrbDMyQ1X5NuN7S2NtxwQ77xjW+8bNnSpUtfNn/AAQdwyy23vGLbQw89lDvvvLOV4dUqIsbSKJgvysw1l5o/HBFbZ+aDEbE18MgQmy7nT0M4ALalMYxD0mhhzq7EnmZJGuWiMVDwXGBxZp45YNWVwJq7YcwEvjfE5v8FHBIRmxUXAB5SLJOknmLRLGlUmDx5MosWLao7jE61D/AB4ICI6C9ehwNfBA6OiLuBg4p5IqIvIv4DoLgA8PPALcXrc2suCpSkddWNOdvhGZI0ymXm9UAMs/rAIdrPBz44YP484LzWRCdJ3cGeZkmSJKmERbMkSZJUwqJZkiRJKmHRLEmSJJXwQkBJTTenf05T9zd72uz13sfq1asZM2bMsPNDyUwykw02sH9B0ujViTkbOi9v+5dA0qjwjW98g7322otp06Zx8skns3r1ajbeeGM+8YlPsMcee3DjjTe+Yv7MM89k6tSpTJ06lX/9138FGjfWnzJlCieccAJTp07lgQceGPnAkqR10m1526JZUtdbvHgxl1xyCTfccAP9/f2MGTOGiy66iGeeeYa3vOUt3Hrrrey7774vm3/1q1/Nf/7nf3LzzTdz00038bWvfY1f/epXANx9993Mnj2b22+/ne23377ms5Ok0acb87bDMyR1vWuuuYYFCxbw5je/GYA//OEPbLnllowZM4b3vOc9L7UbOH/99dfzrne9i4022giAd7/73fz85z/nyCOPZPvtt2fGjBntPxFJ6hHdmLctmiV1vcxk5syZnH766S9bfsYZZ7xs/Nu4ceNKx8MBLyVkSVJrdGPetmiW1PUOPPBAjjrqKE455RS23HJLHn/8cZ5++ukRt3nrW9/KiSeeyKmnnkpmcvnll/P1r3+9TRFrNGn2RVRSL+jGvG3RLKnr7brrrnzhC1/gkEMO4cUXX2Ts2LGcffbZI24zffp0TjzxRPbaay8APvjBD7LnnnuydOnSNkQsSb2tG/N2ZGZbDrQ++vr6cv78+XWHoR521ry7KrU75eCdWhxJZ1q8eDG77LJL3WHUbqj3ISIWZGZfTSHVotdydjN7mpt1qy5pJObsP1mbvO3dMyRJkqQSFs2SJElSCYtmSZIkqYRFs6Sm6IbrI1qp189fUncxZ639e2DRLGm9jRs3jscee6xnk3Bm8thjjzFu3Li6Q5GkUr2es2Hd8ra3nJO03rbddluWLVvGihUr6g6lNuPGjWPbbbetOwxJKmXObljbvG3RLGm9jR07lh122KHuMCRJFZiz103LhmdExHYRcW1E3BERt0fE3xTLPxsRyyOiv3gd3qoYJEmSpGZoZU/zKuATmbkwIsYDCyJiXrHurMw8o4XHliRJkpqmZUVzZj4IPFhMPx0Ri4FtWnU8SZIkqVXacveMiJgM7AncXCz6WETcFhHnRcRmw2wzKyLmR8T8Xh+oLkmSpHq1vGiOiI2By4CPZ+ZTwFeAPwem0eiJ/tJQ22Xm3Mzsy8y+iRMntjpMSZIkaVgtLZojYiyNgvmizPwuQGY+nJmrM/NF4GvAXq2MQZIkSVpfrbx7RgDnAosz88wBy7ce0OxdwKJWxSBJkiQ1QyvvnrEP8AHg1xHRXyz7NHBcREwDElgKnNzCGCRJkqT11sq7Z1wPxBCrrmrVMSVJkqRW8ImAkjTKRcR5wBHAI5k5tVh2CTClaLIp8ERmThti26XA08BqYFVm9rUhZEnqOBbNkjT6nQ98GbhwzYLMfN+a6Yj4EvDkCNvvn5mPtiw6SeoCFs2SNMpl5nXF/fJfobho+73AAW0NSpK6TFsebiJJ6lhvBR7OzLuHWZ/ATyJiQUTMGm4nPpBK0mhn0SxJve044OIR1u+bmdOBw4CPRsTbhmrkA6kkjXYWzZLUoyJiQ+DdwCXDtcnM5cXPR4DL8YFUknqURbMk9a6DgDszc9lQKyNio4gYv2YaOAQfSCWpR1k0S9IoFxEXAzcCUyJiWUScVKw6lkFDMyLi9RGx5n76WwHXR8StwC+BH2bmj9sVtyR1Eu+eIUmjXGYeN8zyE4dY9jvg8GL6XmCPlgYnSV3CnmZJkiSphD3NkiQNY07/nLpDkNQh7GmWJEmSSlg0S5IkSSUsmiVJkqQSFs2SJElSCYtmSZIkqYRFsyRJklTColmSJEkqYdEsSZIklbBoliRJkkpYNEuSJEklLJolSZKkEhbNkiRJUgmLZkmSJKmERbMkSZJUwqJZkiRJKmHRLEmSJJWwaJYkSZJKWDRLkiRJJSyaJUmSpBIWzZIkSVIJi2ZJkiSphEWzJEmSVMKiWZIkSSph0SxJkiSVsGiWJEmSSlg0S9IoFxHnRcQjEbFowLLPRsTyiOgvXocPs+2hEfGbiLgnIk5tX9SS1FksmiVp9DsfOHSI5Wdl5rTiddXglRExBjgbOAzYFTguInZtaaSS1KEsmiVplMvM64DH12HTvYB7MvPezPwj8C3gqKYGJ0ldwqJZknrXxyLitmL4xmZDrN8GeGDA/LJimST1HItmSepNXwH+HJgGPAh8aX12FhGzImJ+RMxfsWJFE8KTpM5i0SxJPSgzH87M1Zn5IvA1GkMxBlsObDdgftti2VD7m5uZfZnZN3HixOYHLEk1s2iWpB4UEVsPmH0XsGiIZrcAO0bEDhHxZ8CxwJXtiE+SOs2GdQcgSWqtiLgY2A/YIiKWAacB+0XENCCBpcDJRdvXA/+RmYdn5qqI+BjwX8AY4LzMvL39ZyBJ9WtZ0RwR2wEXAlvRSMpzM/PfImJz4BJgMo1E/d7M/H2r4pCkXpeZxw2x+Nxh2v4OOHzA/FXAK25HJ0m9ppXDM1YBn8jMXYEZwEeL+3ueClyTmTsC1xTzkiRJUsdqWdGcmQ9m5sJi+mlgMY1bFR0FXFA0uwB4Z6tikCRJkpqhLRcCRsRkYE/gZmCrzHywWPUQjeEbkiRJUsdqedEcERsDlwEfz8ynBq7LzKQx3nmo7bznpyRJkjpCS4vmiBhLo2C+KDO/Wyx+eM2tjoqfjwy1rff8lCRJUqdoWdEcEUHj6uzFmXnmgFVXAjOL6ZnA91oVgyRJktQMrbxP8z7AB4BfR0R/sezTwBeBb0fEScB9wHtbGIMkSZK03lpWNGfm9UAMs/rAVh1XkiRJajYfoy1JkiSVsGiWJEmSSlg0S5IkSSUsmiVJkqQSFs2SJElSCYtmSZIkqYRFsyRJklTColmSJEkqYdEsSZIklbBoliRJkkpYNEuSJEklLJolSZKkEhbNkiRJUgmLZkmSJKmERbMkSZJUwqJZkiRJKmHRLEmSJJXYsO4AJEmSOtVZ8+4qbXPKwTs174DXnl7eZv9PNe94qsyeZkmSJKmERbMkSZJUwqJZkka5iDgvIh6JiEUDlv1LRNwZEbdFxOURsekw2y6NiF9HRH9EzG9b0JLUYSyaJWn0Ox84dNCyecDUzNwduAsYaZDk/pk5LTP7WhSfJHW8SkVzROzW6kAkSeXWJR9n5nXA44OW/SQzVxWzNwHbNiE8SRq1qvY0z4mIX0bE7Ih4bUsjkiSNpBX5+H8BPxpmXQI/iYgFETGrSceTpK5TqWjOzLcC7we2AxZExDcj4uCWRiZJeoVm5+OI+HtgFXDRME32zczpwGHARyPibcPsZ1ZEzI+I+StWrFjXcCSpY1Ue05yZdwP/AHwS+Evg34uLSN7dquAkSa/UrHwcEScCRwDvz8wc5ljLi5+PAJcDew3Tbm5m9mVm38SJE9cmDEnqClXHNO8eEWcBi4EDgHdk5i7F9FktjE+SNECz8nFEHAr8H+DIzHx2mDYbRcT4NdPAIcCiodpK0mhXtaf5/wELgT0y86OZuRAgM39Ho7dDktQea52PI+Ji4EZgSkQsi4iTgC8D44F5xe3kzinavj4irio23Qq4PiJuBX4J/DAzf9zKk5OkTlX1MdpvB/6QmasBImIDYFxmPpuZX29ZdJKkwdY6H2fmcUMsPneYtr8DDi+m7wX2aErUktTlqvY0Xw28esD8a4plkqT2Mh9LUg2qFs3jMnPlmpli+jWtCUmSNALzsSTVoOrwjGciYvqasXMR8SbgD60LS5I0DPOx1IXOmndXpXanVK3MmnC8Uw7eqe376mZVP5qPA5dGxO+AAF4HvK9VQUmShvVxzMeS1HaViubMvCUidgamFIt+k5kvtC4sSdJQzMeSVI+1+RLgzcDkYpvpEUFmXtiSqCRJIzEfS1KbVSqaI+LrwJ8D/cDqYnECJmlJaiPzsSTVo2pPcx+w63CPWZUktY35WJJqUPWWc4toXGwiSaqX+ViSalC1p3kL4I6I+CXw/JqFmXlkS6KSJA3HfCxJNahaNH+2lUFIkir7bN0BSFIvqnrLuf+OiO2BHTPz6oh4DTCmtaFJkgYzH0tSPSqNaY6IDwHfAb5aLNoGuKJFMUmShmE+lqR6VL0Q8KPAPsBTAJl5N7Blq4KSJA3LfCxJNahaND+fmX9cMxMRG9K4L6gkqb3Mx5JUg6pF839HxKeBV0fEwcClwPdbF5YkaRjmY0mqQdWi+VRgBfBr4GTgKuAfRtogIs6LiEciYtGAZZ+NiOUR0V+8Dl/XwCWpR611PpYkrb+qd894Efha8arqfODLvPLRrmdl5hlrsR9JUmEd87G6xJz+OaVtZk+b3YZIJA1WqWiOiN8yxJi5zHzDcNtk5nURMXndQ5MkDbYu+ViStP6qPtykb8D0OOAYYPN1PObHIuIEYD7wicz8/VCNImIWMAtg0qRJ63goSRp1mpmPJUkVVRrTnJmPDXgtz8x/Bd6+Dsf7CvDnwDTgQeBLIxxzbmb2ZWbfxIkT1+FQkjT6NDEfS5LWQtXhGdMHzG5Ao6ejai/1SzLz4QH7/Brwg7XdhyT1smblY0nS2qmaaAf2CK8ClgLvXduDRcTWmflgMfsuYNFI7SVJr9CUfCxJWjtV756x/9ruOCIuBvYDtoiIZcBpwH4RMY3GRSxLadwuSZJU0brkY0nS+qs6PON/j7Q+M88cYtlxQzQ9t2JckqQhrEs+liStv7W5e8abgSuL+XcAvwTubkVQkqRhmY8lqQZVi+ZtgemZ+TQ0nuwH/DAzj29VYJKkIZmPJakGVR+jvRXwxwHzfyyWSZLay3wsSTWo2tN8IfDLiLi8mH8ncEFLIpIkjWSt83FEnAccATySmVOLZZsDlwCTKe7AMdTDpiJiJvAPxewXMtPcL6knVX24yT8Cfw38vnj9dWb+UysDkyS90jrm4/OBQwctOxW4JjN3BK4p5l+mKKxPA94C7AWcFhGbrdcJSFKXqjo8A+A1wFOZ+W/AsojYoUUxSZJGtlb5ODOvAx4ftPgo/tRDfQGNHuvB/icwLzMfL3qh5/HK4luSekKlojkiTgM+CXyqWDQW+EargpIkDa2J+XirAQ+beoihx0VvAzwwYH5ZsUySek7VMc3vAvYEFgJk5u8iYnzLopIkDafp+TgzMyJyffYREbOAWQCTJk1an11J6jAz7p9bodUZLY+jblWHZ/wxM5PGk/yIiI1aF5IkaQTNyscPR8TWxT62Bh4Zos1yYLsB89sWy14hM+dmZl9m9k2cOHEdQ5KkzlW1aP52RHwV2DQiPgRcDXytdWFJkobRrHx8JTCzmJ4JfG+INv8FHBIRmxUXAB5SLJOknlM6PCMigsZtiXYGngKmAJ/JzHktjk2SNMC65uOIuBjYD9giIpbRuCPGF2kU4CcB9wHvLdr2AR/OzA9m5uMR8XnglmJXn8vMwRcUSlJPKC2ai7FuV2XmbjSunJYk1WBd83FmHjfMqgOHaDsf+OCA+fOA89Y2VkkabaoOz1gYEW9uaSSSpCrMx5JUg6p3z3gLcHxELAWeAYJGp8furQpMkjQk87Ek1WDEojkiJmXm/TRucC9Jqon5WJLqVdbTfAUwPTPvi4jLMvM9bYhJkvRKV2A+lqTalI1pjgHTb2hlIJKkEZmPJalGZUVzDjMtSWov87Ek1ahseMYeEfEUjR6OVxfT8KcLTzZpaXSSpDXMx5JUoxGL5swc065AJEnDMx9LUr2q3qdZkiRJ6lkWzZIkSVKJqg83kSSpK8zpn1Op3exps1scSWtUOb9uPbdONOP+uRVandGk/QBvmFDe5trTK+zIu1I2mz3NkiRJUgmLZkmSJKmERbMkSZJUwqJZkiRJKmHRLEmSJJWwaJYkSZJKWDRLkiRJJSyaJUmSpBIWzZIkSVIJi2ZJkiSphEWzJEmSVMKiWZIkSSph0SxJkiSV2LDuACRJkrrZjef+bd0hvMKM++eWN7p2QusDGeCseXeVtjnl4J3aEMm6sadZkiRJKmHRLEmSJJWwaJYkSZJKWDRLUo+KiCkR0T/g9VREfHxQm/0i4skBbT5TU7iSVCsvBJSkHpWZvwGmAUTEGGA5cPkQTX+emUe0MTRJ6jj2NEuSAA4ElmTmfXUHIkmdyKJZkgRwLHDxMOv2johbI+JHEfHGdgYlSZ2iZUVzRJwXEY9ExKIByzaPiHkRcXfxc7NWHV+SVE1E/BlwJHDpEKsXAttn5h7A/wOuGGYfsyJifkTMX7FiRctilaS6tLKn+Xzg0EHLTgWuycwdgWuKeUlSvQ4DFmbmw4NXZOZTmbmymL4KGBsRWwzRbm5m9mVm38SJE1sfsSS1WcuK5sy8Dnh80OKjgAuK6QuAd7bq+JKkyo5jmKEZEfG6iIhiei8afzcea2NsktQR2n33jK0y88Fi+iFgqzYfX5I0QERsBBwMnDxg2YcBMvMc4GjgIxGxCvgDcGxmZh2xSlKdarvlXGZmRAybeCNiFjALYNKkSW2LS5J6SWY+A0wYtOycAdNfBr7c7rgkqdO0++4ZD0fE1gDFz0eGa+j4OEmSJHWKdhfNVwIzi+mZwPfafHxJkiRprbVseEZEXAzsB2wREcuA04AvAt+OiJOA+4D3tur4kiSNZE7/nLpD0Do4a95dldqdcvBOLY5EvaZlRXNmHjfMqgNbdUxJkiSpFXwioCRJklTColmSJEkqYdEsSZIklbBoliRJkkpYNEuSJEklLJolSZKkEhbNkiRJUgmLZkmSJKmERbMkSZJUwqJZkiRJKmHRLEmSJJWwaJYkSZJKWDRLkiRJJSyaJUmSpBIWzZIkSVIJi2ZJkiSpxIZ1B6D6zemf05T9zJ42uyn7kSRJ6jT2NEuSJEklLJolSZKkEhbNkiRJUgmLZkmSJKmERbMkSZJUwqJZkiRJKmHRLEmSJJWwaJakHhYRSyPi1xHRHxHzh1gfEfHvEXFPRNwWEdPriFOS6ubDTSRJ+2fmo8OsOwzYsXi9BfhK8VOSeoo9zZKkkRwFXJgNNwGbRsTWdQclSe1mT7NUwYz751ZseUZL4xjorHl3lbY55eCduvZ4apsEfhIRCXw1Mwf/sm8DPDBgflmx7ME2xSdJHcGiWZJ6276ZuTwitgTmRcSdmXnd2u4kImYBswAmTZrU7Billrjx3L+tO4Ra3XjvY83b2bWnV2j0nuYdrwYOz5CkHpaZy4ufjwCXA3sNarIc2G7A/LbFssH7mZuZfZnZN3HixFaFK0m1sWiWpB4VERtFxPg108AhwKJBza4ETijuojEDeDIzHZohqec4PEOSetdWwOURAY2/B9/MzB9HxIcBMvMc4CrgcOAe4Fngr2uKVZJqZdEsST0qM+8F9hhi+TkDphP4aDvjkqRO5PAMSZIkqYRFsyRJklTColmSJEkqYdEsSZIklbBoliRJkkp49wypmSo9EalZuvvJSpIkdRN7miVJkqQSFs2SJElSCYtmSZIkqYRFsyRJklTCCwElSVLXmHH/3GoNr53Q2kBa5MZ7Hytts/cbuvPcup09zZIkSVKJWnqaI2Ip8DSwGliVmX11xCFJkiRVUefwjP0z89Eajy9JkiRV4vAMSZIkqURdPc0J/CQiEvhqZr5iVH9EzAJmAUyaNKnN4akbnDXvrtI2r5p4dWmb2dNmNyOctqt0MUzVC2H2/9T6BSNJ0ihXV0/zvpk5HTgM+GhEvG1wg8ycm5l9mdk3ceLE9kcoSZIkFWopmjNzefHzEeByYK864pAkSZKqaHvRHBEbRcT4NdPAIcCidschSZIkVVXHmOatgMsjYs3xv5mZP64hDkmSJKmSthfNmXkvsEe7jytJkiStK285J0mSJJWwaJYkSZJKWDRLkiRJJep8jLYkSWqBOf1zStt064Odqrrx3sfqDqGnVHm/Z1D+UK4bzy0/1k2TZlUJiVMO3qlSu6rsaZYkSZJK2NM8ilXpaWj38UZ7z4YkSRqd7GmWJEmSSlg0S1KPiojtIuLaiLgjIm6PiL8Zos1+EfFkRPQXr8/UEask1c3hGZLUu1YBn8jMhRExHlgQEfMy845B7X6emUfUEJ8kdQx7miWpR2Xmg5m5sJh+GlgMbFNvVJLUmSyaJUlExGRgT+DmIVbvHRG3RsSPIuKN7Y1MkjqDwzMkqcdFxMbAZcDHM/OpQasXAttn5sqIOBy4AthxiH3MAmYBTJo0qbUBS1IN7GmWpB4WEWNpFMwXZeZ3B6/PzKcyc2UxfRUwNiK2GKLd3Mzsy8y+iRMntjxuSWo3i2ZJ6lEREcC5wOLMPHOYNq8r2hERe9H4u+Gj1iT1HIdnSFLv2gf4APDriOgvln0amASQmecARwMfiYhVwB+AYzMza4hVkmpl0SxJPSozrweipM2XgS+3JyJJ6lwWzR2o1x9HXfXx3wufKv+GeO+JE8p3dO3plY4nSZJ6l2OaJUmSpBL2NEuSpCFV/eZvNH/72YluvNdrcetgT7MkSZJUwqJZkiRJKmHRLEmSJJWwaJYkSZJKWDRLkiRJJSyaJUmSpBIWzZIkSVIJ79PcZlXveTlaVTr/3/684t52bc6+Nt294vHKzXnittI2s5t4vKap8FTEGfdXuS/oGesfiyRJHcieZkmSJKmERbMkSZJUwqJZkiRJKmHRLEmSJJWwaJYkSZJKWDRLkiRJJbzlnCSpI1S5JeXsabPbEElvaPstUCvc2pL9P9X6ONT1Ztw/t2LL5t4G1Z5mSZIkqYRFsyRJklTColmSJEkqMarHNDdrvFYnjqHr2sdxV35Edrltn1pQ3mjTV5c2qfLo6z3ZpkpIlVQ5XhXNjKkTnTXvrtI2p2x4WXOOteo95cc6eKemHEuS1J3saZYkSZJKWDRLkiRJJSyaJUmSpBIWzZIkSVIJi2ZJkiSphEWzJEmSVKKWojkiDo2I30TEPRFxah0xSJLK83FEvCoiLinW3xwRk2sIU5Jq1/aiOSLGAGcDhwG7AsdFxK7tjkOSel3FfHwS8PvM/AvgLOCf2xulJHWGOnqa9wLuycx7M/OPwLeAo2qIQ5J6XZV8fBRwQTH9HeDAiIg2xihJHaGOonkb4IEB88uKZZKk9qqSj19qk5mrgCeBCW2JTpI6SGRmew8YcTRwaGZ+sJj/APCWzPzYoHazgFnF7BTgN20NtPm2AB6tO4g267Vz9nxHt3U93+0zc2Kzg2mGKvk4IhYVbZYV80uKNo8O2lc7c3a3/e4Zb2sZb+t1W8zrG++QeXvD9djhuloObDdgftti2ctk5lxgbruCarWImJ+ZfXXH0U69ds6e7+g2Ss+3Sj5e02ZZRGwIvBZ4bPCO2pmzu+2zMN7WMt7W67aYWxVvHcMzbgF2jIgdIuLPgGOBK2uIQ5J6XZV8fCUws5g+GvhptvsrSknqAG3vac7MVRHxMeC/gDHAeZl5e7vjkKReN1w+jojPAfMz80rgXODrEXEP8DiNwlqSek4dwzPIzKuAq+o4do1GzVCTtdBr5+z5jm6j8nyHyseZ+ZkB088Bx7Q7rhLd9lkYb2sZb+t1W8wtibftFwJKkiRJ3cbHaEuSJEklLJpbJCI2j4h5EXF38XOzIdpMi4gbI+L2iLgtIt5XR6zroxcfwVvhnP93RNxRfKbXRMT2dcTZLFUfex8R74mIjIiuucJ6KFXONyLeW3zGt0fEN9sdY6+pkk+Ldqsjor94tf0C827LhxXiPTEiVgx4Tz9YR5xFLOdFxCPFLRCHWh8R8e/FudwWEdPbHeMQMZXFvF9EPDng/f3MUO3aISK2i4hrB+S1vxmiTUe9xxVjbu57nJm+WvAC/i9wajF9KvDPQ7TZCdixmH498CCwad2xr8U5jgGWAG8A/gy4Fdh1UJvZwDnF9LHAJXXH3YZz3h94TTH9kW4+5yrnW7QbD1wH3AT01R13iz/fHYFfAZsV81vWHfdof1XJp8W6lR3+u9Mx+bBivCcCX6778y9ieRswHVg0zPrDgR8BAcwAbu6CmPcDflB3nEUsWwPTi+nxwF1D/D501HtcMeamvsf2NLfOwEfPXgC8c3CDzLwrM+8upn8HPAJ05EMQhtGLj+AtPefMvDYzny1mb6Jx79tuVfWx958H/hl4rp3BtUCV8/0QcHZm/h4gMx9pc4y9qDSfdoBuy4dV/213hMy8jsbdW4ZzFHBhNtwEbBoRW7cnuqFViLljZOaDmbmwmH4aWMwrnw7aUe9xxZibyqK5dbbKzAeL6YeArUZqHBF70fjf/pJWB9ZEvfgI3rV9DPxJNP5n3q1Kz7f4im67zPxhOwNrkSqf707AThFxQ0TcFBGHti263lU1n46LiPnF5/LO9oT2km7Lh1Vz2XuKr+K/ExHbDbG+U6xtbu4Ue0fErRHxo4h4Y93BABTDhvYEbh60qmPf4xFihia+x7Xccm60iIirgdcNservB85kZkbEsLcpKf6n9nVgZma+2NwoVZeIOB7oA/6y7lhaJSI2AM6k8TVur9iQxhCN/Wh8i3BdROyWmU/UGVS3a1I+3T4zl0fEG4CfRsSvM7ObOiI6zfeBizPz+Yg4mUYv+QE1xzSaLKTxO7syIg4HrqCRW2oTERsDlwEfz8yn6oylqpKYm/oeWzSvh8w8aLh1EfFwRGydmQ8WRfGQX+FGxCbAD4G/L77u6CZNewRvF6n0GPiIOIjGH/u/zMzn2xRbK5Sd73hgKvCz4lvm1wFXRsSRmTm/bVE2T5XPdxmNsXwvAL+NiLtoJOFb2hPi6NSMfJqZy4uf90bEz2j0PLWraO62fFgab2YOjO0/aIwt71SVcnMnGVjgZeZVETEnIrbIzEfriCcixtIoPi/KzO8O0aTj3uOymJv9Hjs8o3UGPnp2JvC9wQ2i8djay2mMEfpOG2Nrll58BG/pOUfEnsBXgSNHwXjXEc83M5/MzC0yc3JmTqYxhrtbC2ao9jt9BY1eZiJiCxrDNe5tY4y9qEo+3SwiXlVMbwHsA9zRtgi7Lx9WyWUDx6seSWPMaKe6EjihuMPDDODJAUN6OlJEvG7NmPZiiOYG1PSfqCKOc4HFmXnmMM066j2uEnPT3+NmXVHo6xVXdU4ArgHuBq4GNi+W9wH/UUwfD7wA9A94Tas79rU8z8NpXLG6hEZvOcDnaBROAOOAS4F7gF8Cb6g75jac89XAwwM+0yvrjrmV5zuo7c/o4rtnVPx8g8aQlDuAXwPH1h3zaH9VzKf/o/g8bi1+ntSBvzsdlQ8rxHs6cHvxnl4L7FxjrBfTuMPUCzS+7TkJ+DDw4WJ9AGcX5/LrTshDFWL+2ID39ybgf9QY675AArcN+Nt1eCe/xxVjbup77BMBJUmSpBIOz5AkSZJKWDRLkiRJJSyaJUmSpBIWzZIkSVIJi2ZJkiSphEWzJEmSVMKiWZIkSSph0SxJkiSV+P8BmrN2/1VXAP4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y07bv = regDf.target < .7\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,6))\n",
    "regDf[y07bv].plot(kind='hist', ax=axes[0], bins=30, alpha=.5)\n",
    "regDf[~y07bv].plot(kind='hist', ax=axes[1], bins=30, alpha=.5)\n",
    "axes[0].set_title(\"target < 0.7\")\n",
    "axes[1].set_title(\"target > 0.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlierbv = (regDf.error.abs() > .41).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 sklearn 이용방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.24030739 -0.20726607  0.22282854  0.52408311]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.24030738911226024, array([-0.20726607,  0.22282854,  0.52408311]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X, y = bunch.data[:,:3], bunch.data[:,3]\n",
    "\n",
    "lr = LinearRegression().fit(X, y)\n",
    "print(result.params.values)\n",
    "lr.intercept_, lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. classification\n",
    "We consider binary classification, and want to predict not directly <font color='red'>y</font> but <font color='blue'>p(y=1)</font>.\n",
    "\n",
    "We Set:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat y &= P(y=1) = \\sigma(X \\cdot w) \\\\\n",
    "\\sigma(z) &= \\cfrac 1 {1+\\exp(-z)}\\\\\n",
    "\\cfrac {\\partial \\sigma(z)}{\\partial z} &= \\cfrac {\\exp(-z)} {(1+\\exp(-z))^2} = \\sigma(z)(1-\\sigma(z))\\\\\n",
    "Loss(w) &= NLL(w) \n",
    "= - \\left[ y^T \\cdot \\log P(y=1) \\right] - \\left[ (1-y)^T \\cdot \\log (1-P(y=1)) \\right] \\\\\n",
    "\\cfrac {\\partial Loss}{\\partial w} &= - X^T \\cdot \\left[ y * (1 - P(y=1)) \\right] + X^T \\cdot \\left[(1-y) * P(y=1) \\right] \\\\\n",
    "&= X^T \\cdot (\\hat y - y)\n",
    "\\end{align}$$\n",
    "\n",
    "We update w with :\n",
    "$$w(t+1) = w(t) - lr * \\cfrac {\\partial Loss}{\\partial w} \\Bigg|_{w =w(t)}$$\n",
    "\n",
    "---\n",
    "### likelihood vs Loss\n",
    "Likelihood(우도; 그럴싸함)\n",
    "- $L_i = y_i*P(y_i = 1) + (1-y_i)*P(y_i = 0)$ : P(y=0) + P(y=1) = 1\n",
    "\n",
    "$$\\begin{align}L_i &= P(y_i=1)^{y_i} * [1 - P(y_i=1)]^{1-y_i}\\\\\n",
    "L_j &= P(y_j=1)^{y_j} * [1 - P(y_j=1)]^{1-y_j}\\\\\n",
    "L_{ij} &= \\left[ P(y_i=1)^{y_i} * [1 - P(y_i=1)]^{1-y_i} \\right] * \\left[ P(y_j=1)^{y_j} * [1 - P(y_j=1)]^{1-y_j} \\right] \\\\\n",
    "L_{1..n} &= \\Pi_{i=1}^n {P(y_i=1)^{y_i} * [1 - P(y_i=1)]^{1-y_i}} \\\\\n",
    "\\log {L_{1..n}} &= \\sum_{i=1}^n {y_i * \\log P(y_i=1) + (1-y_i) * \\log {[1 - P(y_i=1)]}} \\\\\n",
    "NNL_{i..i} &= - \\sum_{i=1}^n \\left[ {y_i * \\log P(y_i=1) + (1-y_i) * \\log {[1 - P(y_i=1)]}} \\right] \\\\\n",
    "&= - \\left[ y^T \\cdot \\log P(y=1) \\right] - \\left[ (1-y)^T \\cdot \\log (1-P(y=1)) \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "Loss = NLL(Negative Log Likelihood) : CrossEntropy라고도 부르며, 두분포 y, P(y=1)이 얼마나 다른 지를 측정하는 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 statistical tool\n",
    "binary case만을 고려하기 위해, 인위적인 분류문제를 생성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.31629979,  0.70803893, -2.29291305,  0.28220666],\n",
       "       [-0.67690095, -0.77936232, -1.12511459, -0.85295128],\n",
       "       [ 2.70496343,  1.81256131,  0.09005406,  0.62008331],\n",
       "       [ 0.5868326 ,  0.36681905, -0.45731792, -0.46723319],\n",
       "       [ 0.65746416, -1.15725619, -0.03396529, -0.40039299]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_features=4, n_redundant=0, n_informative=1,\n",
    "                           n_clusters_per_class=1, random_state=4)\n",
    "print(X.shape)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.31629979,  0.70803893, -2.29291305,  0.28220666],\n",
       "       [ 1.        , -0.67690095, -0.77936232, -1.12511459, -0.85295128],\n",
       "       [ 1.        ,  2.70496343,  1.81256131,  0.09005406,  0.62008331],\n",
       "       [ 1.        ,  0.5868326 ,  0.36681905, -0.45731792, -0.46723319],\n",
       "       [ 1.        ,  0.65746416, -1.15725619, -0.03396529, -0.40039299]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X)\n",
    "print(X.shape)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.124262\n",
      "         Iterations 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>   100</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    95</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     4</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 22 Jul 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.8207</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:36:10</td>     <th>  Log-Likelihood:    </th> <td> -12.426</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -69.295</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.161e-23</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3037</td> <td>    0.603</td> <td>    0.503</td> <td> 0.615</td> <td>   -0.879</td> <td>    1.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -1.0410</td> <td>    0.731</td> <td>   -1.423</td> <td> 0.155</td> <td>   -2.474</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    5.6889</td> <td>    1.538</td> <td>    3.700</td> <td> 0.000</td> <td>    2.675</td> <td>    8.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.2788</td> <td>    0.553</td> <td>   -0.504</td> <td> 0.614</td> <td>   -1.363</td> <td>    0.805</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.8798</td> <td>    0.659</td> <td>   -1.336</td> <td> 0.182</td> <td>   -2.171</td> <td>    0.411</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.16 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  100\n",
       "Model:                          Logit   Df Residuals:                       95\n",
       "Method:                           MLE   Df Model:                            4\n",
       "Date:                Thu, 22 Jul 2021   Pseudo R-squ.:                  0.8207\n",
       "Time:                        13:36:10   Log-Likelihood:                -12.426\n",
       "converged:                       True   LL-Null:                       -69.295\n",
       "Covariance Type:            nonrobust   LLR p-value:                 1.161e-23\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3037      0.603      0.503      0.615      -0.879       1.486\n",
       "x1            -1.0410      0.731     -1.423      0.155      -2.474       0.392\n",
       "x2             5.6889      1.538      3.700      0.000       2.675       8.703\n",
       "x3            -0.2788      0.553     -0.504      0.614      -1.363       0.805\n",
       "x4            -0.8798      0.659     -1.336      0.182      -2.171       0.411\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.16 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30366162, -1.04095437,  5.68894902, -0.27882206, -0.87983546])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using numpy with linear algebra\n",
    "can't get exact solution, but approximation method with Hessian Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Machine Learning Method\n",
    "#### 1) SGD(확률적 경사하강법)\n",
    "1. 전체 학습데이터의 일부분(batch)을 이용하여 예측을 수행하고\n",
    "2. 예측결과를 실제 값과 구분하여 손실을 계산하고\n",
    "3. 손실을 줄이는 방향으로 학습 파라미터(w)를 업데이트하고\n",
    "4. 1 ~ 3을 전체 학습데이터를 모두 이용할 때까지 반복한 후,\n",
    "5. 학습데이터를 다시 섞고 1 ~ 4의 과정을 EPOCH의 수만큼 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46903266, -0.24571793,  0.41240044,  0.4701742 , -0.25338022])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(234)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 150000 # 50000\n",
    "batch = 34 # 2^n이 좋으나 데이터가 100개라 1/3로 함\n",
    "lrs = [0.0005, 0.00005, 0.00001]\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "MIN = 10**(-12)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    loss = 0\n",
    "    if epoch < 50000:\n",
    "        lr = lrs[0]\n",
    "    elif epoch < 100000:\n",
    "        lr = lrs[1]\n",
    "    else: lr = lrs[2]\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "        # 아래는 직접 구현해보세요.\n",
    "        yhat = sigmoid(x_batch.dot(w))\n",
    "        loss += - y_batch.T.dot(np.log(yhat + MIN)) \\\n",
    "                - (1-y_batch).T.dot(np.log(1-yhat + MIN))\n",
    "        w -= lr*x_batch.T.dot(yhat - y_batch)\n",
    "        \n",
    "    loss /= rows\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\hat y &= P(y=1) = \\sigma(X \\cdot w) \\\\\n",
    "Loss(w) &= NLL(w) \n",
    "= - \\left[ y^T \\cdot \\log P(y=1) \\right] - \\left[ (1-y)^T \\cdot \\log (1-P(y=1)) \\right] \\\\\n",
    "\\cfrac {\\partial Loss}{\\partial w} &= X^T \\cdot (\\hat y - y) = dw \\\\\n",
    "w &= w - \\text{lr}*dw\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30365817, -1.04094798,  5.68892956, -0.27881875, -0.87982798])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30366162, -1.04095437,  5.68894902, -0.27882206, -0.87983546])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모멘텀 기법 사용하기\n",
    "<참고> https://twinw.tistory.com/247\n",
    "$$v(t+1) = m * v(t) - \\alpha \\cfrac {\\partial \\text{Loss}(w)}{\\partial w} \\\\\n",
    "w(t+1) = w(t) + v(t+1)$$\n",
    "\n",
    "```\n",
    "v(t+1) = m * v(t) - a * dW(t)\n",
    "W(t+1) = W(t) + v(t+1)\n",
    "\n",
    "v(0) = 0, m = 0.9\n",
    "v(1) = - a * dW(0)\n",
    "W(1) = W(0) + v(1) = W(0) - a * dW(0)\n",
    "\n",
    "v(2) = m * v(1) - a * dW(1) = - 0.9 * a * dW(0) - a * dW(1)\n",
    "W(2) = W(1) - a * [ 0.9 * dW(0) + dW(1) ]\n",
    "\n",
    "v(3) = m * v(2) - a * dW(2) = - a * [ 0.9 * 0.9 * dW(0) + 0.9 * dW(1) + dW(2) ]\n",
    "W(3) = W(2) + v(3) = W(2) - a * [ 0.9 * 0.9 * dW(0) + 0.9 * dW(1) + dW(2) ]\n",
    "```\n",
    "- a: learning rate\n",
    "- m: momentum. memory of prior velocity. generally 0.9 ~ 0.99\n",
    "- v: velocity. moving speed and direction.\n",
    "\n",
    "```python\n",
    "v = m * v - learning_rate * dW\n",
    "W += v\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19646919, -0.21386067, -0.27314855,  0.05131477,  0.21946897])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(123)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50000 # 50000\n",
    "batch = 34\n",
    "lrs = [0.0005, 0.00005, 0.00001]\n",
    "lr = 0.00005\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "m = 0.95\n",
    "v = np.zeros(w.shape)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    loss = 0\n",
    "    if epoch < 20000:\n",
    "        lr = lrs[0]\n",
    "    elif epoch < 40000:\n",
    "        lr = lrs[1]\n",
    "    else: lr = lrs[2]\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "        yhat = sigmoid(x_batch.dot(w))\n",
    "        loss += - y_batch.T.dot(np.log(yhat + MIN)) \\\n",
    "                - (1-y_batch).T.dot(np.log(1-yhat + MIN))\n",
    "        dw = x_batch.T.dot(yhat - y_batch)\n",
    "        # w -= lr*dw\n",
    "        v = m*v - lr*dw\n",
    "        w += v\n",
    "        \n",
    "    loss /= rows\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30366195, -1.0409778 ,  5.68896112, -0.27881419, -0.8798264 ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogitResults' object has no attribute 'para'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-c0265bda1e04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpara\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\base\\wrapper.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_attrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogitResults' object has no attribute 'para'"
     ]
    }
   ],
   "source": [
    "result.para\n",
    "ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lossdf = pd.DataFrame(losses, columns=[\"loss\"])\n",
    "lossdf[10000:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Nesterov 기법 사용하기\n",
    "![](https://t1.daumcdn.net/cfile/tistory/996E494B5B0D03A003)\n",
    "\n",
    "```python\n",
    "v = m * v - learning_rate * d(w + m*v)\n",
    "w += v\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(234)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50000 # 50000\n",
    "batch = 34\n",
    "lrs = [0.0005, 0.00005, 0.00001, 0.000005]\n",
    "lr = 0.00005\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "m = 0.95\n",
    "v = 0\n",
    "\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    loss = 0\n",
    "    if epoch < 10000:\n",
    "        lr = lrs[0]\n",
    "    elif epoch < 20000:\n",
    "        lr = lrs[1]\n",
    "    elif epoch < 40000:\n",
    "        lr = lrs[2]\n",
    "    else: lr = lrs[3]\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "        yhat = sigmoid(x_batch.dot(w))\n",
    "        loss += - y_batch.T.dot(np.log(yhat + MIN)) \\\n",
    "                - (1-y_batch).T.dot(np.log(1-yhat + MIN))\n",
    "        # you must calculus d(w+m*v)\n",
    "        yhatAhead = sigmoid(x_batch.dot(w+m*v))\n",
    "        dwAhead = x_batch.T.dot(yhatAhead - y_batch)\n",
    "        v = m*v - lr*dwAhead\n",
    "        w += v\n",
    "        \n",
    "    loss /= rows\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossdf = pd.DataFrame(losses, columns=[\"loss\"])\n",
    "lossdf[5000:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) RMSprop 사용하기\n",
    "```python\n",
    "v = m * v + (1 - m) * dw**2\n",
    "w += - learning_rate * dw / (np.sqrt(v) + eps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46903266, -0.24571793,  0.41240044,  0.4701742 , -0.25338022])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(234)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50000 # 50000\n",
    "batch = 34\n",
    "# lrs = [0.0005, 0.00005, 0.00001, 0.000005]\n",
    "lr = 0.0005\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "m = 0.95\n",
    "v = np.zeros(w.shape)\n",
    "eps=1e-10\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "        yhat = sigmoid(x_batch.dot(w))\n",
    "        loss += - y_batch.T.dot(np.log(yhat + MIN)) \\\n",
    "                - (1-y_batch).T.dot(np.log(1-yhat + MIN))\n",
    "        # 아래 사항 구현하세요.\n",
    "        dw = x_batch.T.dot(yhat - y_batch)\n",
    "        v = m * v + (1 - m) * dw**2\n",
    "        w += - lr * dw / (np.sqrt(v) + eps)\n",
    "        \n",
    "    loss /= rows\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30795988, -1.04916538,  5.71735805, -0.2813089 , -0.8835331 ])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30366162, -1.04095437,  5.68894902, -0.27882206, -0.87983546])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.where(sigmoid(X.dot(w)) > .5, 1, 0) == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/995D673359E9AD3537)\n",
    "$$Score(y=1) = X \\cdot w + b$$\n",
    "\n",
    "#### 개념\n",
    "![](https://cgp.iiarjournals.org/content/cgp/15/1/41/F1.medium.gif)\n",
    "\n",
    "여기서 분류평면은 $\\mathbf w^T \\mathbf x + b = 0$으로 표현되며, 분류평면과 평행하며, support vector가 놓인 두개의 평면은 각각 아래 그림과 같이 설정하자.\n",
    "![](http://i.imgur.com/afe8W3S.png)\n",
    "- 분류평면에 임의의 값을 곱해도 그 값은 변하지 않고 0이므로,\n",
    "- 우리는 적절한 값을 곱하여 각 support vector 평면들을 각각 1, -1로 두는 것이 가능하다.\n",
    "\n",
    "분류 평면 위에 놓인 임의의 두 점 $\\mathbf x_1 \\mathbf x_2$은 각각 분류 평면 위에 있으므로, 아래 식을 만족한다.\n",
    "$$\\mathbf w^T \\mathbf x_1 + b = 0 \\quad \\cdots \\quad (1)$$\n",
    "$$\\mathbf w^T \\mathbf x_2 + b = 0 \\quad \\cdots \\quad (2)$$\n",
    "\n",
    "이때 위 식(1)에서 식(2)를 빼면, hyperplane 위에 위치한 임의의 직선 벡터 $\\mathbf x_1 - \\mathbf x_2$에 대해 아래 식이 항상 성립한다.\n",
    "$$\\mathbf w^T (\\mathbf x_1 - \\mathbf x_2) = 0 \\quad \\cdots \\quad (3)$$\n",
    "\n",
    "즉, weights vector $\\mathbf w$는 항상 hyperplane과 수직이다.\n",
    "\n",
    "\n",
    "##### hyperplane 찾기\n",
    "Margine을 최대화하는 hyperplane은 식 (7)를 최적화하는 $\\mathbf w$가 솔루션이다.\n",
    "$$max\\frac { 2 }{ { \\left\\| w \\right\\|  }_{ 2 } } \\rightarrow \\min { \\frac { 1 }{ 2 } { \\left\\| w \\right\\|  }_{ 2 }^{ 2 } } \\quad \\cdots \\quad (7)$$\n",
    "\n",
    "hyperplane을 경계로 위쪽의 samples $\\mathbf x_+$와 아래쪽의 samples $\\mathbf x_-$에 대해 다음식이 성립한다:\n",
    "$$\\mathbf w^T \\mathbf x_+ + b \\ge +1 \\quad \\cdots \\quad (8)$$\n",
    "$$\\mathbf w^T \\mathbf x_- + b \\le -1 \\quad \\cdots \\quad (9)$$\n",
    "\n",
    "또한 이때의 class label y에 대해 $y_+ = +1$, $y_- = -1$이므로, 제약식 (10)이 성립한다.\n",
    "$$y_i (\\mathbf w^T \\mathbf x_i + b) \\ge 1 \\quad \\cdots \\quad (10)$$\n",
    "\n",
    "따라서 모든 i에 대해 식 (10)의 제약조건을 만족하면서, 식 (7)을 최소화하는 $\\mathbf w, b$를 찾으면 된다.\n",
    "\n",
    "Loss는 아래와 같이 계산된다.\n",
    "$$Loss = \\frac {1}{2} {\\left\\| w \\right\\|}_{2}^{2} + C \\sum_{i=1}^n \\max{(0, 1 - \\mathbf y_i * (\\mathbf X_i \\cdot \\mathbf w + b))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46903266, -0.24571793,  0.41240044,  0.4701742 , -0.25338022])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(234)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 2000 # 50000\n",
    "batch = 34\n",
    "lr = 0.0005\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "m = 0.95\n",
    "v = np.zeros(w.shape)\n",
    "eps=1e-10\n",
    "C = 1\n",
    "\n",
    "yprime = np.where(y, y, -1)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = yprime[batch_index]\n",
    "        yhat = x_batch.dot(w)\n",
    "        slacks = 1 - y_batch*yhat\n",
    "        slackIdx = slacks > 0\n",
    "        # loss += .5*w.T.dot(w) + C*np.maximum(0, slacks).sum()\n",
    "        loss += .5*w.T.dot(w) + C*slacks[slackIdx].sum()\n",
    "        # 아래 사항 구현하세요.\n",
    "        # dw = w - ((slackIdx*y_batch).reshape((-1,1))*x_batch).sum(axis=0)\n",
    "        dw = w - C*(x_batch.T*slackIdx*y_batch).sum(axis=1)\n",
    "        v = m * v + (1 - m) * dw**2\n",
    "        w += - lr * dw / (np.sqrt(v) + eps)\n",
    "        \n",
    "    loss /= rows\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0318566 , -0.08741406,  1.57071052,  0.03875019, -0.16422054])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.where(X.dot(w) > 0, 1, 0) == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(losses[1000:]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Quiz] svm을 함수 또는 클래스로 구현하고, 정확도를 이용하여 C를 튜닝해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from numpy.random import shuffle, rand\n",
    "\n",
    "\n",
    "class SVM(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, epochs=2000, C=1., eps=1e-10, seed=234):\n",
    "        self.epochs = epochs\n",
    "        self.C = C\n",
    "        self.eps = eps\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def fit(self, X, y, lr=0.0005, batch=34, m=.95):\n",
    "\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.losses = []\n",
    "        self.w = rand(self.X_.shape[-1]) - 0.5\n",
    "        self.lr = lr\n",
    "        self.m = m\n",
    "        self.yprime = np.where(self.y_, self.y_, -1)\n",
    "        self.v = np.zeros(self.w.shape)\n",
    "        rows = self.X_.shape[0]\n",
    "        randRow = np.arange(rows)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            shuffle(randRow)\n",
    "            loss = 0\n",
    "\n",
    "            for i in range(0, rows, batch):\n",
    "                batch_index = randRow[i:i+batch]\n",
    "                x_batch = self.X_[batch_index]\n",
    "                y_batch = self.yprime[batch_index]\n",
    "                yhat = x_batch.dot(self.w)\n",
    "                slacks = 1 - y_batch*yhat\n",
    "                slackIdx = slacks > 0\n",
    "                loss += .5*self.w.T.dot(self.w) + self.C*slacks[slackIdx].sum()\n",
    "                dw = self.w - self.C*(x_batch.T*slackIdx*y_batch).sum(axis=1)\n",
    "                self.v = self.m * self.v + (1 - self.m) * dw**2\n",
    "                self.w += - self.lr * dw / (np.sqrt(self.v) + self.eps)\n",
    "\n",
    "            loss /= rows\n",
    "            self.losses.append(loss)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        return np.where(X.dot(self.w)>0, 1, 0)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return (np.where(X.dot(self.w)>0, 1, 0) == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVM(C=0.000001).fit(X, y)\n",
    "svm.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(svm.predict(X) == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current score is 0.52 at C=1e-05\n",
      "current best score is 0.52 at C=1e-05\n",
      "current score is 0.96 at C=0.0001\n",
      "current best score is 0.96 at C=0.0001\n",
      "current score is 0.96 at C=0.001\n",
      "current score is 0.96 at C=0.01\n",
      "current score is 0.96 at C=0.1\n",
      "current score is 0.96 at C=1\n",
      "current score is 0.92 at C=10\n",
      "current score is 0.92 at C=100\n",
      "current score is 0.92 at C=1000\n",
      "current score is 0.92 at C=10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.96,\n",
       " 'm': SVM(C=0.0001),\n",
       " 'C': 0.0001,\n",
       " 'losses': [0.014632761889432244,\n",
       "  0.014321913704399405,\n",
       "  0.014109565777513157,\n",
       "  0.013933036791894959,\n",
       "  0.013776451580527887,\n",
       "  0.013632797331121755,\n",
       "  0.013498258477232572,\n",
       "  0.0133705042627491,\n",
       "  0.01324799638746687,\n",
       "  0.013129662286944378,\n",
       "  0.013014720733056083,\n",
       "  0.012902592744248634,\n",
       "  0.012792839320887888,\n",
       "  0.012685100986980767,\n",
       "  0.012579103259750068,\n",
       "  0.012474624795596655,\n",
       "  0.01237148718156428,\n",
       "  0.012269542067493094,\n",
       "  0.012168669978841562,\n",
       "  0.012068772272781808,\n",
       "  0.011969766085786093,\n",
       "  0.011871580352320442,\n",
       "  0.011774155392979702,\n",
       "  0.011677444098029458,\n",
       "  0.011581405113370401,\n",
       "  0.011486005024024548,\n",
       "  0.01139121712253068,\n",
       "  0.011297009151474332,\n",
       "  0.011203361948627614,\n",
       "  0.01111025659344716,\n",
       "  0.011017686184321874,\n",
       "  0.010925631730212122,\n",
       "  0.01083408305739939,\n",
       "  0.010743025907393781,\n",
       "  0.010652453086950564,\n",
       "  0.010562354753746879,\n",
       "  0.010472725071390264,\n",
       "  0.01038356476454666,\n",
       "  0.010294873011298573,\n",
       "  0.01020663616864158,\n",
       "  0.010118850463033943,\n",
       "  0.010031514740331437,\n",
       "  0.009944625924572726,\n",
       "  0.009858182043844007,\n",
       "  0.009772180513775925,\n",
       "  0.009686620664985194,\n",
       "  0.009601501649364498,\n",
       "  0.009516822979501937,\n",
       "  0.009432582528662536,\n",
       "  0.009348778511640241,\n",
       "  0.009265411593375043,\n",
       "  0.009182482063314047,\n",
       "  0.009099986510220709,\n",
       "  0.009017926943274558,\n",
       "  0.008936301866233969,\n",
       "  0.008855117929923878,\n",
       "  0.008774380268321634,\n",
       "  0.008694078720166058,\n",
       "  0.00861420814124259,\n",
       "  0.008534776214994206,\n",
       "  0.008455788096659372,\n",
       "  0.008377235332043658,\n",
       "  0.00829911224718586,\n",
       "  0.008221417928150744,\n",
       "  0.008144154517724204,\n",
       "  0.008067321314292473,\n",
       "  0.007990917884972538,\n",
       "  0.007914946365401326,\n",
       "  0.007839404440692194,\n",
       "  0.0077642907519222485,\n",
       "  0.007689611446435909,\n",
       "  0.007615382858535131,\n",
       "  0.007541584618575161,\n",
       "  0.007468214328515259,\n",
       "  0.007395281010420607,\n",
       "  0.007322779463865672,\n",
       "  0.0072507054288093575,\n",
       "  0.007179059299075347,\n",
       "  0.00710783983700164,\n",
       "  0.007037047533740342,\n",
       "  0.006966682155124976,\n",
       "  0.0068967444541206575,\n",
       "  0.006827234299957984,\n",
       "  0.0067581542534860765,\n",
       "  0.006689502284351347,\n",
       "  0.006621275729046494,\n",
       "  0.006553476543790956,\n",
       "  0.006486103552951734,\n",
       "  0.00641915715070744,\n",
       "  0.006352637346235012,\n",
       "  0.006286544218918129,\n",
       "  0.006220878628191903,\n",
       "  0.006155639295203218,\n",
       "  0.00609082368185587,\n",
       "  0.006026434422087168,\n",
       "  0.005962471688987416,\n",
       "  0.00589893241078657,\n",
       "  0.005835818104294739,\n",
       "  0.005773136568989094,\n",
       "  0.0057108999797984275,\n",
       "  0.005649100858857225,\n",
       "  0.0055877308141769775,\n",
       "  0.005526782171122501,\n",
       "  0.005466253343419438,\n",
       "  0.00540614580573007,\n",
       "  0.00534645976039443,\n",
       "  0.005287194881307378,\n",
       "  0.005228350561074525,\n",
       "  0.005169926685929944,\n",
       "  0.0051119227875255704,\n",
       "  0.005054338147840499,\n",
       "  0.004997174481003516,\n",
       "  0.004940430186246528,\n",
       "  0.004884105691171441,\n",
       "  0.004828200856085485,\n",
       "  0.004772715340110935,\n",
       "  0.004717647396873487,\n",
       "  0.004662998692176412,\n",
       "  0.004608767957548926,\n",
       "  0.004554954654809949,\n",
       "  0.004501560673534673,\n",
       "  0.004448582469191145,\n",
       "  0.004396017435584914,\n",
       "  0.004343869437883534,\n",
       "  0.004292137175049809,\n",
       "  0.004240820104030858,\n",
       "  0.004189916395214308,\n",
       "  0.0041394263313544125,\n",
       "  0.004089349459467707,\n",
       "  0.0040396836700415445,\n",
       "  0.003990430653202531,\n",
       "  0.0039415895431241296,\n",
       "  0.003893158260500082,\n",
       "  0.003845136438511026,\n",
       "  0.0037975231506507303,\n",
       "  0.003750319579600645,\n",
       "  0.0037035227981854777,\n",
       "  0.003657132775113472,\n",
       "  0.00361114870417941,\n",
       "  0.003565568480162109,\n",
       "  0.0035203935576750747,\n",
       "  0.003475622810499902,\n",
       "  0.003431254879935775,\n",
       "  0.0033872878599257405,\n",
       "  0.0033437210977549457,\n",
       "  0.0033005543534551495,\n",
       "  0.003257793683896679,\n",
       "  0.0032154409922273536,\n",
       "  0.0031734826841035607,\n",
       "  0.0031319179371534756,\n",
       "  0.0030907453203761707,\n",
       "  0.003049961824155122,\n",
       "  0.0030095666845586016,\n",
       "  0.0029695595507893756,\n",
       "  0.0029299397747401157,\n",
       "  0.0028907036739099327,\n",
       "  0.002851848609265371,\n",
       "  0.0028133741951564006,\n",
       "  0.002775278865653492,\n",
       "  0.0027375591904505257,\n",
       "  0.0027002138940863817,\n",
       "  0.0026632406982535018,\n",
       "  0.0026266358298647824,\n",
       "  0.0025903961504848815,\n",
       "  0.002554520575197979,\n",
       "  0.002519005966319541,\n",
       "  0.002483848060900968,\n",
       "  0.002449044206350825,\n",
       "  0.002414590780461658,\n",
       "  0.002380482758547178,\n",
       "  0.002346717740475611,\n",
       "  0.002313291662057675,\n",
       "  0.002280199275734447,\n",
       "  0.002247435956279067,\n",
       "  0.0022149967406357497,\n",
       "  0.0021828774927775242,\n",
       "  0.002151073693656866,\n",
       "  0.002119579844245151,\n",
       "  0.0020883894040817307,\n",
       "  0.002057497217938315,\n",
       "  0.0020268989055611942,\n",
       "  0.001996589366666847,\n",
       "  0.001966566281117232,\n",
       "  0.0019368209840513477,\n",
       "  0.0019073517914277452,\n",
       "  0.001878153311745773,\n",
       "  0.0018492229947597264,\n",
       "  0.0018205572186064467,\n",
       "  0.001792153786924596,\n",
       "  0.0017640108529188854,\n",
       "  0.0017361263676051552,\n",
       "  0.0017084986537420784,\n",
       "  0.0016811262034411815,\n",
       "  0.0016540096797056617,\n",
       "  0.0016271462306590209,\n",
       "  0.0016005374942830218,\n",
       "  0.0015741797791568428,\n",
       "  0.0015480776813401032,\n",
       "  0.0015222262625119735,\n",
       "  0.001496629107672268,\n",
       "  0.0014712854073110956,\n",
       "  0.0014461958481418308,\n",
       "  0.0014213521940060591,\n",
       "  0.001396765369194338,\n",
       "  0.0013724283826998217,\n",
       "  0.0013483440766030841,\n",
       "  0.0013245157052441464,\n",
       "  0.0013009336957143678,\n",
       "  0.0012776038871393973,\n",
       "  0.0012545204011748592,\n",
       "  0.0012316938489250035,\n",
       "  0.0012091188225038293,\n",
       "  0.0011867882323112109,\n",
       "  0.0011647080692369653,\n",
       "  0.0011428779202840948,\n",
       "  0.0011213077903487005,\n",
       "  0.0010999719717524159,\n",
       "  0.001078889466679169,\n",
       "  0.001058056988864497,\n",
       "  0.0010374748695449805,\n",
       "  0.0010171400666849988,\n",
       "  0.0009970540388005522,\n",
       "  0.0009772093854520788,\n",
       "  0.0009576184752118759,\n",
       "  0.0009382832426407981,\n",
       "  0.0009191756579904384,\n",
       "  0.0009003275741157304,\n",
       "  0.0008817192687981513,\n",
       "  0.0008633643970638908,\n",
       "  0.0008452447506888064,\n",
       "  0.0008273815571458431,\n",
       "  0.0008097633395923684,\n",
       "  0.0007923965408943188,\n",
       "  0.0007752488530523694,\n",
       "  0.0007583532631174593,\n",
       "  0.0007417045656214039,\n",
       "  0.0007252998082932735,\n",
       "  0.0007091433940564561,\n",
       "  0.0006932242981428888,\n",
       "  0.0006775430031938203,\n",
       "  0.0006621176910998313,\n",
       "  0.0006469176097279085,\n",
       "  0.0006319714639719306,\n",
       "  0.0006172470399874049,\n",
       "  0.0006027662965568703,\n",
       "  0.0005885360260937752,\n",
       "  0.0005745308689175044,\n",
       "  0.0005607758643477165,\n",
       "  0.0005472509342161595,\n",
       "  0.0005339711299502868,\n",
       "  0.0005209183328703511,\n",
       "  0.000508109573152576,\n",
       "  0.0004955208100510233,\n",
       "  0.0004831707596714672,\n",
       "  0.0004710552585891142,\n",
       "  0.00045917235796371747,\n",
       "  0.0004475320718178159,\n",
       "  0.0004361121421756677,\n",
       "  0.0004249191978915313,\n",
       "  0.0004139581880087632,\n",
       "  0.0004032434330020578,\n",
       "  0.00039273038962860395,\n",
       "  0.00038244388021990087,\n",
       "  0.0003723929478573363,\n",
       "  0.00036257184044691774,\n",
       "  0.0003529572438470884,\n",
       "  0.0003435725970963439,\n",
       "  0.0003344122130446046,\n",
       "  0.00032547534651830806,\n",
       "  0.00031673544044343884,\n",
       "  0.0003082500345614054,\n",
       "  0.0002999250178497053,\n",
       "  0.00029184164340856074,\n",
       "  0.0002839735030279306,\n",
       "  0.00027630431467926464,\n",
       "  0.0002688545092599848,\n",
       "  0.00026159760384674086,\n",
       "  0.00025456180161616455,\n",
       "  0.0002477051188832195,\n",
       "  0.0002410647776745071,\n",
       "  0.00023460647392262588,\n",
       "  0.00022836120511142796,\n",
       "  0.00022230456645815652,\n",
       "  0.00021640551932687325,\n",
       "  0.00021071414266879472,\n",
       "  0.00020520361636492047,\n",
       "  0.00019987304278603263,\n",
       "  0.00019474924020628683,\n",
       "  0.00018973551868527982,\n",
       "  0.00018492561028162047,\n",
       "  0.00018028350177548376,\n",
       "  0.00017579165169914244,\n",
       "  0.00017146136239286653,\n",
       "  0.0001672973486255732,\n",
       "  0.00016327266581336473,\n",
       "  0.00015940794114917864,\n",
       "  0.0001556904475532758,\n",
       "  0.00015210584470848893,\n",
       "  0.00014867987752143258,\n",
       "  0.0001453737171759221,\n",
       "  0.00014221799983144838,\n",
       "  0.00013919982825759747,\n",
       "  0.0001363189837838654,\n",
       "  0.0001335472954609832,\n",
       "  0.00013092137166841497,\n",
       "  0.00012841792686955387,\n",
       "  0.00012604506674403202,\n",
       "  0.00012380179807778017,\n",
       "  0.00012167631272763486,\n",
       "  0.00011967545012131836,\n",
       "  0.0001177974585220738,\n",
       "  0.00011602810251146012,\n",
       "  0.00011437267779496038,\n",
       "  0.00011282393469549158,\n",
       "  0.00011139739180121717,\n",
       "  0.00011007264607432114,\n",
       "  0.00010886002583212673,\n",
       "  0.00010773608623656527,\n",
       "  0.00010671147821569765,\n",
       "  0.00010576905117294369,\n",
       "  0.00010492911168744419,\n",
       "  0.00010418212037801462,\n",
       "  0.0001034991356958604,\n",
       "  0.00010290187288484705,\n",
       "  0.00010237660959126386,\n",
       "  0.00010193066755457719,\n",
       "  0.00010154154496875447,\n",
       "  0.00010119090035953801,\n",
       "  0.00010091198597065382,\n",
       "  0.00010067658382418036,\n",
       "  0.00010049355527660034,\n",
       "  0.00010034874147409523,\n",
       "  0.00010022497870080392,\n",
       "  0.00010012741246028901,\n",
       "  0.00010005458492652764,\n",
       "  0.000100008940080264,\n",
       "  9.997179926648066e-05,\n",
       "  9.993828987584288e-05,\n",
       "  9.992774927341315e-05,\n",
       "  9.99173552056233e-05,\n",
       "  9.990392785104016e-05,\n",
       "  9.991175997439415e-05,\n",
       "  9.989376573098279e-05,\n",
       "  9.989659101485773e-05,\n",
       "  9.989701504007671e-05,\n",
       "  9.989092181152356e-05,\n",
       "  9.990526020947801e-05,\n",
       "  9.990090523019766e-05,\n",
       "  9.990838235600912e-05,\n",
       "  9.990388711045237e-05,\n",
       "  9.989398274960801e-05,\n",
       "  9.99010580289516e-05,\n",
       "  9.989366709819957e-05,\n",
       "  9.989855334035021e-05,\n",
       "  9.989833372360108e-05,\n",
       "  9.989043786478437e-05,\n",
       "  9.989762365033028e-05,\n",
       "  9.989264078736406e-05,\n",
       "  9.990184039372453e-05,\n",
       "  9.990036023000415e-05,\n",
       "  9.990573960267389e-05,\n",
       "  9.990592287408785e-05,\n",
       "  9.991105632012468e-05,\n",
       "  9.990177875110611e-05,\n",
       "  9.989840427829598e-05,\n",
       "  9.989561343716396e-05,\n",
       "  9.990024970490999e-05,\n",
       "  9.989196367786132e-05,\n",
       "  9.99147906954829e-05,\n",
       "  9.990254373475663e-05,\n",
       "  9.992060731205262e-05,\n",
       "  9.990509120711583e-05,\n",
       "  9.989688373232462e-05,\n",
       "  9.98998610170528e-05,\n",
       "  9.989975337688796e-05,\n",
       "  9.989794978957857e-05,\n",
       "  9.989824219356292e-05,\n",
       "  9.989621891115406e-05,\n",
       "  9.989582574894508e-05,\n",
       "  9.990684863410537e-05,\n",
       "  9.990542212795286e-05,\n",
       "  9.99095564942371e-05,\n",
       "  9.990096432852665e-05,\n",
       "  9.99032473079369e-05,\n",
       "  9.990044571968187e-05,\n",
       "  9.992528481697181e-05,\n",
       "  9.990476581739062e-05,\n",
       "  9.991304891451123e-05,\n",
       "  9.989612728274568e-05,\n",
       "  9.990684878414093e-05,\n",
       "  9.98981491258355e-05,\n",
       "  9.990015296446935e-05,\n",
       "  9.993970000032049e-05,\n",
       "  9.990581004982732e-05,\n",
       "  9.990641800861094e-05,\n",
       "  9.992105887549528e-05,\n",
       "  9.99088890179575e-05,\n",
       "  9.990708955736518e-05,\n",
       "  9.990205526276818e-05,\n",
       "  9.990339501174263e-05,\n",
       "  9.990324448404288e-05,\n",
       "  9.990983942835721e-05,\n",
       "  9.990583336778593e-05,\n",
       "  9.99089718962982e-05,\n",
       "  9.990428828154013e-05,\n",
       "  9.992215532799856e-05,\n",
       "  9.989356658682551e-05,\n",
       "  9.990323946019428e-05,\n",
       "  9.989321171396643e-05,\n",
       "  9.989666639413393e-05,\n",
       "  9.989760328073876e-05,\n",
       "  9.990813702470505e-05,\n",
       "  9.989739771626951e-05,\n",
       "  9.99062258524912e-05,\n",
       "  9.992011409033031e-05,\n",
       "  9.990208178598501e-05,\n",
       "  9.990295039855401e-05,\n",
       "  9.991941609601324e-05,\n",
       "  9.992336129704908e-05,\n",
       "  9.990958413650152e-05,\n",
       "  9.989531591232323e-05,\n",
       "  9.992938600932613e-05,\n",
       "  9.990129501696828e-05,\n",
       "  9.989844510601964e-05,\n",
       "  9.991676686082927e-05,\n",
       "  9.990531696320206e-05,\n",
       "  9.99072218011999e-05,\n",
       "  9.992688107297335e-05,\n",
       "  9.990510645589825e-05,\n",
       "  9.991037652575565e-05,\n",
       "  9.990366419864118e-05,\n",
       "  9.991301223092285e-05,\n",
       "  9.990619513557789e-05,\n",
       "  9.98961645777898e-05,\n",
       "  9.989937393394026e-05,\n",
       "  9.991709580812243e-05,\n",
       "  9.990341850816327e-05,\n",
       "  9.991244938261563e-05,\n",
       "  9.989916280926524e-05,\n",
       "  9.990895622195317e-05,\n",
       "  9.992129887882666e-05,\n",
       "  9.991470482036919e-05,\n",
       "  9.991504088721092e-05,\n",
       "  9.990145237106486e-05,\n",
       "  9.990009578750999e-05,\n",
       "  9.990476809658956e-05,\n",
       "  9.990149173725347e-05,\n",
       "  9.989557252331937e-05,\n",
       "  9.991007955592039e-05,\n",
       "  9.991290365201503e-05,\n",
       "  9.99096631539322e-05,\n",
       "  9.989773038681743e-05,\n",
       "  9.990540825420197e-05,\n",
       "  9.989637008314369e-05,\n",
       "  9.991257460043627e-05,\n",
       "  9.990636672375263e-05,\n",
       "  9.99089814564148e-05,\n",
       "  9.990412443284197e-05,\n",
       "  9.990223211204013e-05,\n",
       "  9.989916069635202e-05,\n",
       "  9.990868210736941e-05,\n",
       "  9.989985278342887e-05,\n",
       "  9.991449930344388e-05,\n",
       "  9.991162458107948e-05,\n",
       "  9.990450635531389e-05,\n",
       "  9.990876760481624e-05,\n",
       "  9.990929039732813e-05,\n",
       "  9.990966748381711e-05,\n",
       "  9.990178741928036e-05,\n",
       "  9.991603601490677e-05,\n",
       "  9.990774271711702e-05,\n",
       "  9.990231899375928e-05,\n",
       "  9.991881661551127e-05,\n",
       "  9.991100201630088e-05,\n",
       "  9.989980892524638e-05,\n",
       "  9.990325462440082e-05,\n",
       "  9.991105333699832e-05,\n",
       "  9.990602049037113e-05,\n",
       "  9.990541642637453e-05,\n",
       "  9.99057363783531e-05,\n",
       "  9.991043643203035e-05,\n",
       "  9.990101281767966e-05,\n",
       "  9.990126773883672e-05,\n",
       "  9.991139574730484e-05,\n",
       "  9.989595452915428e-05,\n",
       "  9.990593150999461e-05,\n",
       "  9.990178960341262e-05,\n",
       "  9.989668389345239e-05,\n",
       "  9.989787808488864e-05,\n",
       "  9.99279568258771e-05,\n",
       "  9.991095735373066e-05,\n",
       "  9.990037391631291e-05,\n",
       "  9.989887375365999e-05,\n",
       "  9.989706131431727e-05,\n",
       "  9.990539741886665e-05,\n",
       "  9.991060352046898e-05,\n",
       "  9.990137156975587e-05,\n",
       "  9.990619480096114e-05,\n",
       "  9.990384576536414e-05,\n",
       "  9.990956486775361e-05,\n",
       "  9.992301094705217e-05,\n",
       "  9.990199238882111e-05,\n",
       "  9.991742436899274e-05,\n",
       "  9.991057352233707e-05,\n",
       "  9.990567540473052e-05,\n",
       "  9.989646219768401e-05,\n",
       "  9.990935228774515e-05,\n",
       "  9.991817012601728e-05,\n",
       "  9.989747075494057e-05,\n",
       "  9.990389423701681e-05,\n",
       "  9.990967904693582e-05,\n",
       "  9.991406045111973e-05,\n",
       "  9.990471784677429e-05,\n",
       "  9.991932377033933e-05,\n",
       "  9.990084214250772e-05,\n",
       "  9.990767935535022e-05,\n",
       "  9.99055587491186e-05,\n",
       "  9.990469182770637e-05,\n",
       "  9.990009975764747e-05,\n",
       "  9.990309356594744e-05,\n",
       "  9.991307570306205e-05,\n",
       "  9.991021461162622e-05,\n",
       "  9.989786671546114e-05,\n",
       "  9.991444693477605e-05,\n",
       "  9.99088828358255e-05,\n",
       "  9.989844300353472e-05,\n",
       "  9.989575859140338e-05,\n",
       "  9.991680321965248e-05,\n",
       "  9.990236651352264e-05,\n",
       "  9.990138888286752e-05,\n",
       "  9.990595576205504e-05,\n",
       "  9.989587809693401e-05,\n",
       "  9.990008956502943e-05,\n",
       "  9.990872319990093e-05,\n",
       "  9.990404399029946e-05,\n",
       "  9.990447743002486e-05,\n",
       "  9.991314950783419e-05,\n",
       "  9.991747349881497e-05,\n",
       "  9.991721700665932e-05,\n",
       "  9.990458254272121e-05,\n",
       "  9.99025942494835e-05,\n",
       "  9.990674801826113e-05,\n",
       "  9.991613466129158e-05,\n",
       "  9.989847864043802e-05,\n",
       "  9.989700944220763e-05,\n",
       "  9.990046865277082e-05,\n",
       "  9.990238743677757e-05,\n",
       "  9.990095784651409e-05,\n",
       "  9.990891270539118e-05,\n",
       "  9.990850618631657e-05,\n",
       "  9.989401706673629e-05,\n",
       "  9.990897787902028e-05,\n",
       "  9.990862813705766e-05,\n",
       "  9.98953555166533e-05,\n",
       "  9.991531714133473e-05,\n",
       "  9.990991545567628e-05,\n",
       "  9.99091809358275e-05,\n",
       "  9.989982583238024e-05,\n",
       "  9.989865522842058e-05,\n",
       "  9.990670413445452e-05,\n",
       "  9.990065357579472e-05,\n",
       "  9.990229182547891e-05,\n",
       "  9.990858934360538e-05,\n",
       "  9.990141377915118e-05,\n",
       "  9.990445448000339e-05,\n",
       "  9.990869236370544e-05,\n",
       "  9.992141530737686e-05,\n",
       "  9.99073815287119e-05,\n",
       "  9.99064374876977e-05,\n",
       "  9.989759882431692e-05,\n",
       "  9.98964334909351e-05,\n",
       "  9.99240612792871e-05,\n",
       "  9.991211757724736e-05,\n",
       "  9.989637983098282e-05,\n",
       "  9.991281310393412e-05,\n",
       "  9.989687199751583e-05,\n",
       "  9.991959040021609e-05,\n",
       "  9.990076079290825e-05,\n",
       "  9.990009499452116e-05,\n",
       "  9.990733800723004e-05,\n",
       "  9.991000729925632e-05,\n",
       "  9.990228826819938e-05,\n",
       "  9.989541736035353e-05,\n",
       "  9.989742673263868e-05,\n",
       "  9.990648755256907e-05,\n",
       "  9.989585091582453e-05,\n",
       "  9.991817630574096e-05,\n",
       "  9.992870425688487e-05,\n",
       "  9.990287267197324e-05,\n",
       "  9.99084676321658e-05,\n",
       "  9.990798341194197e-05,\n",
       "  9.990640978895416e-05,\n",
       "  9.990588908932302e-05,\n",
       "  9.989945579285512e-05,\n",
       "  9.990754518948388e-05,\n",
       "  9.989640217867835e-05,\n",
       "  9.990710172206392e-05,\n",
       "  9.992855620098704e-05,\n",
       "  9.990775672678573e-05,\n",
       "  9.991333421817853e-05,\n",
       "  9.991593124787746e-05,\n",
       "  9.990587317143118e-05,\n",
       "  9.989894318190703e-05,\n",
       "  9.993589712431937e-05,\n",
       "  9.99138092279011e-05,\n",
       "  9.989939154075534e-05,\n",
       "  9.990274653919303e-05,\n",
       "  9.98958201184124e-05,\n",
       "  9.991811481201608e-05,\n",
       "  9.990237048584637e-05,\n",
       "  9.990669946100516e-05,\n",
       "  9.989108183977275e-05,\n",
       "  9.990227390331432e-05,\n",
       "  9.990934009638013e-05,\n",
       "  9.991440733107819e-05,\n",
       "  9.990502720875683e-05,\n",
       "  9.989790167378973e-05,\n",
       "  9.990451369548694e-05,\n",
       "  9.991297589251746e-05,\n",
       "  9.989636034736515e-05,\n",
       "  9.990360178801683e-05,\n",
       "  9.991033054074381e-05,\n",
       "  9.990450085453962e-05,\n",
       "  9.989771049157727e-05,\n",
       "  9.991207888719168e-05,\n",
       "  9.989852343407725e-05,\n",
       "  9.991250524408798e-05,\n",
       "  9.990318395770653e-05,\n",
       "  9.991399505505599e-05,\n",
       "  9.990319964822555e-05,\n",
       "  9.989971611369227e-05,\n",
       "  9.99170062284096e-05,\n",
       "  9.991367683336639e-05,\n",
       "  9.990677743162746e-05,\n",
       "  9.990285738425877e-05,\n",
       "  9.990211291286919e-05,\n",
       "  9.99047546154433e-05,\n",
       "  9.989465127693173e-05,\n",
       "  9.990337282613853e-05,\n",
       "  9.989238920315706e-05,\n",
       "  9.991342208355626e-05,\n",
       "  9.99039451219483e-05,\n",
       "  9.990815485378288e-05,\n",
       "  9.990331713500736e-05,\n",
       "  9.990467309374412e-05,\n",
       "  9.990776813986777e-05,\n",
       "  9.98994483801581e-05,\n",
       "  9.992223844388769e-05,\n",
       "  9.992246365936536e-05,\n",
       "  9.99063915206893e-05,\n",
       "  9.991032295819428e-05,\n",
       "  9.990349172924053e-05,\n",
       "  9.990870364933916e-05,\n",
       "  9.989492934989293e-05,\n",
       "  9.9894487669358e-05,\n",
       "  9.991000805286089e-05,\n",
       "  9.990447215561059e-05,\n",
       "  9.990204057808423e-05,\n",
       "  9.990409671663567e-05,\n",
       "  9.990431325189983e-05,\n",
       "  9.991985546388537e-05,\n",
       "  9.989232827002203e-05,\n",
       "  9.990286283403761e-05,\n",
       "  9.990691387455135e-05,\n",
       "  9.993048003883824e-05,\n",
       "  9.990561840855014e-05,\n",
       "  9.990685978204846e-05,\n",
       "  9.991551310942385e-05,\n",
       "  9.990153353596419e-05,\n",
       "  9.991158217226573e-05,\n",
       "  9.990950462343192e-05,\n",
       "  9.991987248583548e-05,\n",
       "  9.990529326073185e-05,\n",
       "  9.989600067355524e-05,\n",
       "  9.990561608753623e-05,\n",
       "  9.99033231963917e-05,\n",
       "  9.991493628386475e-05,\n",
       "  9.99050116229947e-05,\n",
       "  9.991461872163186e-05,\n",
       "  9.991351931134114e-05,\n",
       "  9.990792808053952e-05,\n",
       "  9.99026250437957e-05,\n",
       "  9.990703639192776e-05,\n",
       "  9.990644371713435e-05,\n",
       "  9.991962140893213e-05,\n",
       "  9.99126168655503e-05,\n",
       "  9.990310110126914e-05,\n",
       "  9.989777747694785e-05,\n",
       "  9.990719885372586e-05,\n",
       "  9.992166979064803e-05,\n",
       "  9.991797308076148e-05,\n",
       "  9.989911305745142e-05,\n",
       "  9.99128538191562e-05,\n",
       "  9.991812026241397e-05,\n",
       "  9.990079425909681e-05,\n",
       "  9.989698289961667e-05,\n",
       "  9.990533230711141e-05,\n",
       "  9.989879414003238e-05,\n",
       "  9.990522366644574e-05,\n",
       "  9.990332262192624e-05,\n",
       "  9.990913412212977e-05,\n",
       "  9.990144723269594e-05,\n",
       "  9.989904901707093e-05,\n",
       "  9.989829102692238e-05,\n",
       "  9.991039652744389e-05,\n",
       "  9.990238564823055e-05,\n",
       "  9.990899555351543e-05,\n",
       "  9.990295640688584e-05,\n",
       "  9.990176937590064e-05,\n",
       "  9.991142955951346e-05,\n",
       "  9.991315663246636e-05,\n",
       "  9.990220125005524e-05,\n",
       "  9.990163913875439e-05,\n",
       "  9.991189082837548e-05,\n",
       "  9.991854152461488e-05,\n",
       "  9.99309477592088e-05,\n",
       "  9.989595275106202e-05,\n",
       "  9.990494839959306e-05,\n",
       "  9.989824101550578e-05,\n",
       "  9.990724968025868e-05,\n",
       "  9.992576431899644e-05,\n",
       "  9.989731152941444e-05,\n",
       "  9.989821243688183e-05,\n",
       "  9.990094779846555e-05,\n",
       "  9.991198748771818e-05,\n",
       "  9.990553471495776e-05,\n",
       "  9.990615593141545e-05,\n",
       "  9.99045633941211e-05,\n",
       "  9.989899097894551e-05,\n",
       "  9.990726103512315e-05,\n",
       "  9.990753662397052e-05,\n",
       "  9.991021278874943e-05,\n",
       "  9.990921179791582e-05,\n",
       "  9.991118822898104e-05,\n",
       "  9.990275114821639e-05,\n",
       "  9.990491628380778e-05,\n",
       "  9.99085578104082e-05,\n",
       "  9.990794005011637e-05,\n",
       "  9.990259788124429e-05,\n",
       "  9.990896849781777e-05,\n",
       "  9.990871591306821e-05,\n",
       "  9.990710851649259e-05,\n",
       "  9.990686712908772e-05,\n",
       "  9.990412123281218e-05,\n",
       "  9.992112851518621e-05,\n",
       "  9.990504765557035e-05,\n",
       "  9.989581899547286e-05,\n",
       "  9.990493830632336e-05,\n",
       "  9.989449254206192e-05,\n",
       "  9.990041687185129e-05,\n",
       "  9.989768007144414e-05,\n",
       "  9.989959414692286e-05,\n",
       "  9.990466053068967e-05,\n",
       "  9.99262782665566e-05,\n",
       "  9.989616015940257e-05,\n",
       "  9.991170886416534e-05,\n",
       "  9.99061347148822e-05,\n",
       "  9.9918260345799e-05,\n",
       "  9.991000425898025e-05,\n",
       "  9.992584299698156e-05,\n",
       "  9.990610766385225e-05,\n",
       "  9.990094906936771e-05,\n",
       "  9.990255119247947e-05,\n",
       "  9.990201243808749e-05,\n",
       "  9.990961752977967e-05,\n",
       "  9.99119874972775e-05,\n",
       "  9.990325354683298e-05,\n",
       "  9.990587509722721e-05,\n",
       "  9.992961774388333e-05,\n",
       "  9.990256190771814e-05,\n",
       "  9.990346399286593e-05,\n",
       "  9.989399079665932e-05,\n",
       "  9.990795742619322e-05,\n",
       "  9.990413103708882e-05,\n",
       "  9.990771193595509e-05,\n",
       "  9.989752787762839e-05,\n",
       "  9.990133841155322e-05,\n",
       "  9.991887862575748e-05,\n",
       "  9.9908762054184e-05,\n",
       "  9.990846547757632e-05,\n",
       "  9.990533261537697e-05,\n",
       "  9.990037541634939e-05,\n",
       "  9.98907321083274e-05,\n",
       "  9.991932470132831e-05,\n",
       "  9.99089444598727e-05,\n",
       "  9.990015587999441e-05,\n",
       "  9.990553475884429e-05,\n",
       "  9.991274143007916e-05,\n",
       "  9.990310019975809e-05,\n",
       "  9.989810429884984e-05,\n",
       "  9.99131413279891e-05,\n",
       "  9.991575648318089e-05,\n",
       "  9.991549307402462e-05,\n",
       "  9.989882355173745e-05,\n",
       "  9.991477740637069e-05,\n",
       "  9.989652972155383e-05,\n",
       "  9.991296532350374e-05,\n",
       "  9.990036212090603e-05,\n",
       "  9.99122998334274e-05,\n",
       "  9.989909614811407e-05,\n",
       "  9.99078981566807e-05,\n",
       "  9.990651026883625e-05,\n",
       "  9.991463731787432e-05,\n",
       "  9.991858638065404e-05,\n",
       "  9.9923849044501e-05,\n",
       "  9.991320115896931e-05,\n",
       "  9.991306625751483e-05,\n",
       "  9.990616200225384e-05,\n",
       "  9.990946641740782e-05,\n",
       "  9.992005699368991e-05,\n",
       "  9.990446948467901e-05,\n",
       "  9.991206972063774e-05,\n",
       "  9.990238536661002e-05,\n",
       "  9.990121892820406e-05,\n",
       "  9.991360664736047e-05,\n",
       "  9.991925440007845e-05,\n",
       "  9.98968470168156e-05,\n",
       "  9.990510307164654e-05,\n",
       "  9.990869755134837e-05,\n",
       "  9.990592051124016e-05,\n",
       "  9.989480636496192e-05,\n",
       "  9.991102957107267e-05,\n",
       "  9.989769540687802e-05,\n",
       "  9.989972789453287e-05,\n",
       "  9.991808707565723e-05,\n",
       "  9.990082984468004e-05,\n",
       "  9.991504831857632e-05,\n",
       "  9.991693242423351e-05,\n",
       "  9.990580288987146e-05,\n",
       "  9.991200464389821e-05,\n",
       "  9.990760627522247e-05,\n",
       "  9.990338303090869e-05,\n",
       "  9.99019345450765e-05,\n",
       "  9.990627308676027e-05,\n",
       "  9.989715450515754e-05,\n",
       "  9.991400497210192e-05,\n",
       "  9.992152308553572e-05,\n",
       "  9.993378806474857e-05,\n",
       "  9.990753608546742e-05,\n",
       "  9.989797251225566e-05,\n",
       "  9.989138527158662e-05,\n",
       "  9.989408056181745e-05,\n",
       "  9.990234316126482e-05,\n",
       "  9.990648440367073e-05,\n",
       "  9.992087434160187e-05,\n",
       "  9.99400121328106e-05,\n",
       "  9.990578089694609e-05,\n",
       "  9.98955935835367e-05,\n",
       "  9.989869048066986e-05,\n",
       "  9.990216719825819e-05,\n",
       "  9.989787284873393e-05,\n",
       "  9.990966887424559e-05,\n",
       "  9.991171002835279e-05,\n",
       "  9.992034902325402e-05,\n",
       "  9.989870675717313e-05,\n",
       "  9.992519586635824e-05,\n",
       "  9.991151028550192e-05,\n",
       "  9.990109031661404e-05,\n",
       "  9.989400306545672e-05,\n",
       "  9.989895441199702e-05,\n",
       "  9.989931169389014e-05,\n",
       "  9.991000488458134e-05,\n",
       "  9.9904228383019e-05,\n",
       "  9.989576696258386e-05,\n",
       "  9.990886215412384e-05,\n",
       "  9.99065530333184e-05,\n",
       "  9.990773775687877e-05,\n",
       "  9.98954368260138e-05,\n",
       "  9.991814336476762e-05,\n",
       "  9.990761552900841e-05,\n",
       "  9.991967510542593e-05,\n",
       "  9.990646017653689e-05,\n",
       "  9.993029970699887e-05,\n",
       "  9.990862869368662e-05,\n",
       "  9.991068896548638e-05,\n",
       "  9.989951533285007e-05,\n",
       "  9.99048435387e-05,\n",
       "  9.989764217265962e-05,\n",
       "  9.99060002309448e-05,\n",
       "  9.992355937268454e-05,\n",
       "  9.990376045172088e-05,\n",
       "  9.990246240804861e-05,\n",
       "  9.98968828509467e-05,\n",
       "  9.990057044246108e-05,\n",
       "  9.989649706133905e-05,\n",
       "  9.989382330950896e-05,\n",
       "  9.991383114845067e-05,\n",
       "  9.989691810883369e-05,\n",
       "  9.990087652823682e-05,\n",
       "  9.990713479296549e-05,\n",
       "  9.990929420442876e-05,\n",
       "  9.993014653458343e-05,\n",
       "  9.991721246227692e-05,\n",
       "  9.99145659359338e-05,\n",
       "  9.991305797969165e-05,\n",
       "  9.990752376237243e-05,\n",
       "  9.990076282846189e-05,\n",
       "  9.990668105969759e-05,\n",
       "  9.989765521659005e-05,\n",
       "  9.99008073402518e-05,\n",
       "  9.991102505685626e-05,\n",
       "  9.990163704831782e-05,\n",
       "  9.991552710361989e-05,\n",
       "  9.990952318213437e-05,\n",
       "  9.989748405358805e-05,\n",
       "  9.990089070053528e-05,\n",
       "  9.991129748040434e-05,\n",
       "  9.991882017953035e-05,\n",
       "  9.990214330656531e-05,\n",
       "  9.991375027266626e-05,\n",
       "  9.989714029300189e-05,\n",
       "  9.989943190391935e-05,\n",
       "  9.989619615916514e-05,\n",
       "  9.992714618417132e-05,\n",
       "  9.991315933860694e-05,\n",
       "  9.990170886350086e-05,\n",
       "  9.990094452005408e-05,\n",
       "  9.990147046974447e-05,\n",
       "  9.990612701803879e-05,\n",
       "  9.990599376361912e-05,\n",
       "  9.991209885848268e-05,\n",
       "  9.989471690042581e-05,\n",
       "  9.991932362259834e-05,\n",
       "  9.989840112747179e-05,\n",
       "  9.990420038053322e-05,\n",
       "  9.992085945467662e-05,\n",
       "  9.990778939285595e-05,\n",
       "  9.991716309891622e-05,\n",
       "  9.989928284135215e-05,\n",
       "  9.990923211217229e-05,\n",
       "  9.98988187037607e-05,\n",
       "  9.989569530969933e-05,\n",
       "  9.990506779685927e-05,\n",
       "  9.990292722433882e-05,\n",
       "  9.99282614022624e-05,\n",
       "  9.989681258419749e-05,\n",
       "  9.991232058663385e-05,\n",
       "  9.990711072125584e-05,\n",
       "  9.989528996565226e-05,\n",
       "  9.990673462780304e-05,\n",
       "  9.991704110133037e-05,\n",
       "  9.989853564480788e-05,\n",
       "  9.991285053499323e-05,\n",
       "  9.990551592159222e-05,\n",
       "  9.989768400708522e-05,\n",
       "  9.990903914893889e-05,\n",
       "  9.99168603287456e-05,\n",
       "  9.989951054282733e-05,\n",
       "  9.990654311326853e-05,\n",
       "  9.990824628092628e-05,\n",
       "  9.990909633143237e-05,\n",
       "  9.989909873078749e-05,\n",
       "  9.990161204572112e-05,\n",
       "  9.989445891322157e-05,\n",
       "  9.991375983078075e-05,\n",
       "  9.989292583167905e-05,\n",
       "  9.993411957790492e-05,\n",
       "  9.99073057393599e-05,\n",
       "  9.99098281341692e-05,\n",
       "  9.989707891976902e-05,\n",
       "  9.990060402166259e-05,\n",
       "  9.990032098693643e-05,\n",
       "  9.991103078114554e-05,\n",
       "  9.990423209260474e-05,\n",
       "  9.990243638736494e-05,\n",
       "  9.992635780075443e-05,\n",
       "  9.99180963016571e-05,\n",
       "  9.990192556239164e-05,\n",
       "  9.990083425640318e-05,\n",
       "  9.991019089721077e-05,\n",
       "  9.990741778433528e-05,\n",
       "  9.990027699707873e-05,\n",
       "  9.990975631418944e-05,\n",
       "  9.990757899892437e-05,\n",
       "  9.98978065915111e-05,\n",
       "  9.991015396365581e-05,\n",
       "  9.990654789288879e-05,\n",
       "  9.989455625414084e-05,\n",
       "  9.991392751197439e-05,\n",
       "  9.99003256845035e-05,\n",
       "  9.989488959295159e-05,\n",
       "  9.989818266803533e-05,\n",
       "  9.990300922707576e-05,\n",
       "  9.991371314517734e-05,\n",
       "  9.990935476544359e-05,\n",
       "  9.991611171460336e-05,\n",
       "  9.990501702587538e-05,\n",
       "  9.990593570246591e-05,\n",
       "  9.992059697685264e-05,\n",
       "  9.991612437459727e-05,\n",
       "  9.991309384745629e-05,\n",
       "  9.990172861119677e-05,\n",
       "  9.99075578013671e-05,\n",
       "  9.990365902514234e-05,\n",
       "  9.989449134634088e-05,\n",
       "  9.991730012903359e-05,\n",
       "  9.99103680959959e-05,\n",
       "  9.990817831299601e-05,\n",
       "  9.991369823512051e-05,\n",
       "  9.990819871724189e-05,\n",
       "  ...]}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=234)\n",
    "\n",
    "best = {}\n",
    "best['score'] = 0\n",
    "for c in [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]:\n",
    "    m = SVM(C=c).fit(X_train, y_train)\n",
    "    s = m.score(X_test, y_test)\n",
    "    print(f\"current score is {s} at C={c}\")\n",
    "    if s > best['score']:\n",
    "        print(f\"current best score is {s} at C={c}\")\n",
    "        best['m'] = m\n",
    "        best['score'] = s\n",
    "        best['C'] = c\n",
    "        best['losses'] = m.losses\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Margin과 C-SVM\n",
    "이제까지 엄격히 두 개의 클래스를 구분하는 결정 경계선을 구하는 Hard Margin 방법을 사용했다. 그러나 현실에서는 아래 그림과 같이 선형 경계로는 두 개의 클래스를 완벽하게 결정할 수 없는 경우가 대부분이다.\n",
    "![](http://i.imgur.com/5OVa7IM.png)\n",
    "\n",
    "이경우 2가지의 해결방법이 있는데:\n",
    "1. Support Vectors와 hyperplane 사이. 즉, 마진 내에 오분류될 수 있는 관측치를 허용하는 Soft Margin.\n",
    "2. 다음에 설명할 선형 결정경계를 사용하지 않고, 올록볼록한 결정경계를 갖는 Kernel Trick.\n",
    "\n",
    "여기서는 먼저 아래 그림과 같이 Soft Margin을 사용하는 C-SVM 기법을 설명하겠다.\n",
    "![](http://i.imgur.com/vlG124W.png)\n",
    "\n",
    "위 그림에서 minus-plane을 벗어난 빨간점과 plus-plane을 벗어난 파란점이 보인다.(이러한 점들을 slack이라 부른다.) 마진을 최대화하되 이런 관측치들을 허용하는 게 바로 C-SVM이다. \n",
    "\n",
    "이를 수식으로 표현하면 아래와 같다:\n",
    "$${ y }_{ i }({ w }^{ T }{ x }_{ i }+b)\\ge 1-{ \\xi  }_{ i },\\quad { \\xi  }_{ i }\\ge 0 \\quad \\cdots \\quad (15)$$\n",
    "- 서포트 벡터가 위치한 경계선을 두고 $\\xi_i$ 만큼의 오류를 인정한다는 의미이다.\n",
    "- 위 그림에서 파란색 slack과 + plane과의 거리가 $l_+$라면:\n",
    " - $1: \\cfrac 1 {|\\mathbf w|} = \\xi_+ : l_+$이므로, $\\xi_+ = l_+ |\\mathbf w|$ 이다.\n",
    "- 역시 빨간색 slack과 - plane과의 거리rk $l_-$인 경우, $\\xi_- = l_- |\\mathbf w|$ 이다.\n",
    "\n",
    "이제 우리는 2개 항을 최소로 유지해야 한다:\n",
    "1. Margine을 최대화 하는 것으로부터: $\\min { \\frac 1 2 { \\left\\| \\mathbf w \\right\\|  }_{ 2 }^{ 2 } }$\n",
    "2. slack points' length를 최소화하기 위해: $\\min \\sum_i \\xi_i$\n",
    "\n",
    "각 최적화 항에 대한 가중치 계수로 $C$를 도입하면 식 (7)은 다음과 같이 쓸 수 있다:\n",
    "$$\\min \\frac 1 2 ||\\mathbf w ||_2^2 + C \\sum_i \\xi_i \\quad \\cdots \\quad (16)$$\n",
    "- 이때의 제약조건은 식 (15)이다.\n",
    "- 이 식은 아래와 같이 L2 loss를 나타내는 $\\alpha$로 다음과 같이 정식화할 수 있다.\n",
    "- $\\min \\frac 1 2 \\alpha ||\\mathbf w ||_2^2 + \\sum_i \\xi_i$, 여기서 $\\alpha = \\cfrac 1 C$이다.\n",
    "\n",
    "이를 L2 loss와 결부하여 해석하면, Margine을 최대화하는 것과 Slack points를 최소화하는 것은 서로 trade-off 관계가 있으므로, 이를 가중치 계수 C로 조절하는 데:\n",
    "- C가 커지면, Margine 최대화보다는 이상치로 볼 수 있는 slack points를 줄이고, 이상치를 정상적인 관측치라 보겠다는 전략이고, \n",
    "- 반대로 C가 1 보다 작으면, Margine을 최대화하여 일반화를 추구하겠다는 의미이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current score is 0.34210526315789475 at C=1e-05\n",
      "current best score is 0.34210526315789475 at C=1e-05\n",
      "current score is 0.34210526315789475 at C=0.0001\n",
      "current score is 0.34210526315789475 at C=0.001\n",
      "current score is 0.34210526315789475 at C=0.01\n",
      "current score is 0.6842105263157895 at C=0.1\n",
      "current best score is 0.6842105263157895 at C=0.1\n",
      "current score is 0.6842105263157895 at C=1\n",
      "current score is 0.6842105263157895 at C=10\n",
      "current score is 0.6842105263157895 at C=100\n",
      "current score is 0.6842105263157895 at C=1000\n",
      "current score is 0.6842105263157895 at C=10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6842105263157895,\n",
       " 'm': SVM(C=0.1),\n",
       " 'C': 0.1,\n",
       " 'losses': [0.14911880549135792,\n",
       "  0.1416070469478839,\n",
       "  0.13629692602546042,\n",
       "  0.13180581733974286,\n",
       "  0.12777051720028096,\n",
       "  0.12398599878708785,\n",
       "  0.12038114788069507,\n",
       "  0.11693061999909506,\n",
       "  0.1135583354975669,\n",
       "  0.11027298721392283,\n",
       "  0.10701117728395142,\n",
       "  0.10384819523995516,\n",
       "  0.10069284618481268,\n",
       "  0.09759474200196305,\n",
       "  0.09459918441858024,\n",
       "  0.09162031997830855,\n",
       "  0.08868784332657619,\n",
       "  0.08568208501892985,\n",
       "  0.08282812840261296,\n",
       "  0.08009978303250673,\n",
       "  0.07730957953231014,\n",
       "  0.07458509588786237,\n",
       "  0.07196587384455497,\n",
       "  0.0695988115483268,\n",
       "  0.06733422038238226,\n",
       "  0.06526346079037001,\n",
       "  0.06339073762413149,\n",
       "  0.061578474223204206,\n",
       "  0.05979173406985412,\n",
       "  0.058149852192646075,\n",
       "  0.05665688148540073,\n",
       "  0.05513867174947732,\n",
       "  0.05374947025673551,\n",
       "  0.05252265796940251,\n",
       "  0.05133312860931817,\n",
       "  0.05020168384118552,\n",
       "  0.04907601931209724,\n",
       "  0.047981161283151386,\n",
       "  0.04695121294251957,\n",
       "  0.04601455319689691,\n",
       "  0.04511231297466791,\n",
       "  0.044324011603293634,\n",
       "  0.04357589386557404,\n",
       "  0.0428748727272933,\n",
       "  0.042166791536553956,\n",
       "  0.04148666347376262,\n",
       "  0.04082871569723101,\n",
       "  0.0402248016442739,\n",
       "  0.039591198429795464,\n",
       "  0.03894069152260963,\n",
       "  0.03827019416576961,\n",
       "  0.0375664962078859,\n",
       "  0.036860538515825096,\n",
       "  0.0361291005442946,\n",
       "  0.03539398428582579,\n",
       "  0.03465562897399832,\n",
       "  0.03388389890945824,\n",
       "  0.03312369689763571,\n",
       "  0.032364653912434226,\n",
       "  0.03157812012293993,\n",
       "  0.030860649753871927,\n",
       "  0.030103738268411513,\n",
       "  0.029388872658151894,\n",
       "  0.028673431109340983,\n",
       "  0.027978604468646036,\n",
       "  0.0272626494078962,\n",
       "  0.02657967457108375,\n",
       "  0.025977261637972145,\n",
       "  0.025408636503866024,\n",
       "  0.024854453620255196,\n",
       "  0.024321909408836336,\n",
       "  0.023762383057295933,\n",
       "  0.023234759155237473,\n",
       "  0.02269380737735417,\n",
       "  0.02218452388269525,\n",
       "  0.02172963660729374,\n",
       "  0.02146637483069139,\n",
       "  0.021239896782606228,\n",
       "  0.02106579984006571,\n",
       "  0.020864850156677637,\n",
       "  0.020712813909778888,\n",
       "  0.020527347840584496,\n",
       "  0.020347235820749026,\n",
       "  0.020223173652410522,\n",
       "  0.02009386403137901,\n",
       "  0.019977828083470477,\n",
       "  0.019887386933787533,\n",
       "  0.01983251818032624,\n",
       "  0.019774419307336137,\n",
       "  0.01972715388150042,\n",
       "  0.019699162144046153,\n",
       "  0.019637279674103034,\n",
       "  0.019600486287016137,\n",
       "  0.019578357464589722,\n",
       "  0.01954739908402619,\n",
       "  0.019525754278361913,\n",
       "  0.019472068497274276,\n",
       "  0.019459440057602147,\n",
       "  0.019411761472470857,\n",
       "  0.019370424404856097,\n",
       "  0.019389660473170985,\n",
       "  0.019323073904389188,\n",
       "  0.019331301422154556,\n",
       "  0.0192841858524969,\n",
       "  0.019219494918250948,\n",
       "  0.019212570156691287,\n",
       "  0.01917309592246575,\n",
       "  0.01914033956162947,\n",
       "  0.01911720325201825,\n",
       "  0.019112324582037033,\n",
       "  0.01906572597078144,\n",
       "  0.01902185008856006,\n",
       "  0.019007883120536747,\n",
       "  0.018993927791249813,\n",
       "  0.018938296900789804,\n",
       "  0.01891589719672124,\n",
       "  0.018886655998451494,\n",
       "  0.018895619279976846,\n",
       "  0.01884382802635482,\n",
       "  0.01878853101589128,\n",
       "  0.018776605090648057,\n",
       "  0.018739337577482217,\n",
       "  0.018728943838708118,\n",
       "  0.018678439498384627,\n",
       "  0.0186580892735399,\n",
       "  0.018621769757724702,\n",
       "  0.01858442584498984,\n",
       "  0.018575804220967333,\n",
       "  0.018554551750416554,\n",
       "  0.018515603902761774,\n",
       "  0.0184783260153735,\n",
       "  0.018524297833969982,\n",
       "  0.018461705849834368,\n",
       "  0.01840772327884471,\n",
       "  0.01837197286231953,\n",
       "  0.01839028208318915,\n",
       "  0.01833303083612622,\n",
       "  0.0183313735362029,\n",
       "  0.018308906692741385,\n",
       "  0.01826481247759337,\n",
       "  0.018235749466352407,\n",
       "  0.018245776268044436,\n",
       "  0.018221952373121214,\n",
       "  0.01816926829082388,\n",
       "  0.01816276279734704,\n",
       "  0.01812723903869665,\n",
       "  0.018097718307203945,\n",
       "  0.018083987787246517,\n",
       "  0.018045244166293602,\n",
       "  0.018031648994743316,\n",
       "  0.01801963739074817,\n",
       "  0.017994888008636134,\n",
       "  0.01798398504542693,\n",
       "  0.017986262820101677,\n",
       "  0.017906900843133018,\n",
       "  0.01788340831581376,\n",
       "  0.017904872946746756,\n",
       "  0.017840881134992815,\n",
       "  0.017814482012348646,\n",
       "  0.01779531160443657,\n",
       "  0.0177974231985536,\n",
       "  0.017773115537639928,\n",
       "  0.017727767719221365,\n",
       "  0.01773439045004144,\n",
       "  0.01769074040373225,\n",
       "  0.017689632271062745,\n",
       "  0.017692553586069255,\n",
       "  0.01762685199698906,\n",
       "  0.017609474260534753,\n",
       "  0.01764072414411506,\n",
       "  0.01760768452308692,\n",
       "  0.017567060639832602,\n",
       "  0.017551647216835258,\n",
       "  0.017520389924300474,\n",
       "  0.017502604993463983,\n",
       "  0.017486965000888137,\n",
       "  0.017540416667058512,\n",
       "  0.01747481789965357,\n",
       "  0.017428046329427963,\n",
       "  0.017417691278803334,\n",
       "  0.017390266835445912,\n",
       "  0.017375089256539376,\n",
       "  0.017380333480862355,\n",
       "  0.017347159419639987,\n",
       "  0.01731847445275125,\n",
       "  0.017329960677566304,\n",
       "  0.017276571313896157,\n",
       "  0.017266184168859732,\n",
       "  0.01725881327875717,\n",
       "  0.017305068999731693,\n",
       "  0.01721306581755481,\n",
       "  0.01720867305464486,\n",
       "  0.017181927162205884,\n",
       "  0.017223276205758708,\n",
       "  0.01715447562036669,\n",
       "  0.01712672588629916,\n",
       "  0.017108865048129987,\n",
       "  0.017111039964005228,\n",
       "  0.017098662207901755,\n",
       "  0.01712614935735654,\n",
       "  0.017044278240052342,\n",
       "  0.01706220812935682,\n",
       "  0.017024355330532957,\n",
       "  0.016999430112416992,\n",
       "  0.017002523050948525,\n",
       "  0.0169774861788751,\n",
       "  0.01696023107045723,\n",
       "  0.016957329765291445,\n",
       "  0.01694921163221817,\n",
       "  0.01691143889378327,\n",
       "  0.01691550685663817,\n",
       "  0.01689704613825404,\n",
       "  0.016861732775732993,\n",
       "  0.016916759617535547,\n",
       "  0.016888843725000952,\n",
       "  0.01685581193598335,\n",
       "  0.016809670432303067,\n",
       "  0.016816532365990566,\n",
       "  0.01680083374546051,\n",
       "  0.016806604597168383,\n",
       "  0.016767319089722257,\n",
       "  0.01678364297024538,\n",
       "  0.016759290213062716,\n",
       "  0.016737019442496386,\n",
       "  0.016732847778040027,\n",
       "  0.01670792256796467,\n",
       "  0.016690025481186897,\n",
       "  0.01669869522762937,\n",
       "  0.01666278804668014,\n",
       "  0.016648633648067495,\n",
       "  0.016693740530410495,\n",
       "  0.016632965371165185,\n",
       "  0.016633255660535244,\n",
       "  0.01659965682036587,\n",
       "  0.016604674480958943,\n",
       "  0.01660219568073135,\n",
       "  0.016566181651184692,\n",
       "  0.016585590660167338,\n",
       "  0.01658498794407149,\n",
       "  0.016528148133750913,\n",
       "  0.016528886583922063,\n",
       "  0.01654365605831488,\n",
       "  0.01650194869482489,\n",
       "  0.016520074405902523,\n",
       "  0.016478866470220604,\n",
       "  0.016479325331004258,\n",
       "  0.016459661420897557,\n",
       "  0.016456080152802723,\n",
       "  0.016443775342523705,\n",
       "  0.016473146010540316,\n",
       "  0.01640713211594642,\n",
       "  0.016402673982685665,\n",
       "  0.016405652058235774,\n",
       "  0.016377738611993174,\n",
       "  0.01637591243213005,\n",
       "  0.016351583567993603,\n",
       "  0.01638396073847272,\n",
       "  0.01634578042608324,\n",
       "  0.016326984493367135,\n",
       "  0.01631771203281538,\n",
       "  0.016304794220416165,\n",
       "  0.016330716238218055,\n",
       "  0.01634104127709991,\n",
       "  0.01628755272588791,\n",
       "  0.016281537609450666,\n",
       "  0.016266468398634982,\n",
       "  0.01625569279973181,\n",
       "  0.01625693388913258,\n",
       "  0.01623890533817055,\n",
       "  0.01623457728826898,\n",
       "  0.016215308998237327,\n",
       "  0.016231313865568208,\n",
       "  0.016196324186764224,\n",
       "  0.016196039177094636,\n",
       "  0.016180110827740665,\n",
       "  0.016189706091361302,\n",
       "  0.016168251836822977,\n",
       "  0.016223105161886183,\n",
       "  0.016136405340064233,\n",
       "  0.016129954817889147,\n",
       "  0.016147783332320885,\n",
       "  0.016127897894863824,\n",
       "  0.016118257347043634,\n",
       "  0.0161239530584524,\n",
       "  0.016105513993367064,\n",
       "  0.016082603469634072,\n",
       "  0.016077501107503327,\n",
       "  0.016134218009650165,\n",
       "  0.01605572436492294,\n",
       "  0.016054757253107555,\n",
       "  0.016080737051285817,\n",
       "  0.016037951107900895,\n",
       "  0.0160681299610177,\n",
       "  0.016070726996508966,\n",
       "  0.016046937370923633,\n",
       "  0.016034146077868957,\n",
       "  0.01600990017662299,\n",
       "  0.01600683121784352,\n",
       "  0.016008075141080456,\n",
       "  0.016012130366893643,\n",
       "  0.015978698561603683,\n",
       "  0.01597472147879078,\n",
       "  0.015991784737349166,\n",
       "  0.016048236041723153,\n",
       "  0.015972290918549983,\n",
       "  0.01596954653336709,\n",
       "  0.015945420239996486,\n",
       "  0.015973405490901118,\n",
       "  0.015928778273606534,\n",
       "  0.015937179253357966,\n",
       "  0.015923343465344998,\n",
       "  0.015925151667694683,\n",
       "  0.015905567771744362,\n",
       "  0.01592796459243926,\n",
       "  0.015931952374133126,\n",
       "  0.015893268229387872,\n",
       "  0.015893929278442184,\n",
       "  0.015891515018947554,\n",
       "  0.01590478924829336,\n",
       "  0.01589809227439512,\n",
       "  0.015858664166689972,\n",
       "  0.01590463093905636,\n",
       "  0.015857211382493165,\n",
       "  0.015840685035521506,\n",
       "  0.015846877895040083,\n",
       "  0.015831535817906698,\n",
       "  0.015832490490459507,\n",
       "  0.01581901575816359,\n",
       "  0.015813414043050825,\n",
       "  0.01582992105684512,\n",
       "  0.015839671775597093,\n",
       "  0.01581736445510851,\n",
       "  0.01581112514839322,\n",
       "  0.01579765314512725,\n",
       "  0.01579103693742558,\n",
       "  0.01581219019964398,\n",
       "  0.015797449052981628,\n",
       "  0.015839690807819173,\n",
       "  0.015761379045884896,\n",
       "  0.015751196229369335,\n",
       "  0.015747803233945378,\n",
       "  0.015757581604045078,\n",
       "  0.015760379063256596,\n",
       "  0.015744061305635246,\n",
       "  0.015730989164247804,\n",
       "  0.015776353234552714,\n",
       "  0.015732907611516054,\n",
       "  0.01571571519738522,\n",
       "  0.015705808735463296,\n",
       "  0.015709194711565608,\n",
       "  0.01569950657348682,\n",
       "  0.01573986764584754,\n",
       "  0.015721694953736015,\n",
       "  0.015704607829194726,\n",
       "  0.015697322251748722,\n",
       "  0.01567312862530496,\n",
       "  0.015672602044796584,\n",
       "  0.01569664438929702,\n",
       "  0.015677918080372204,\n",
       "  0.01567177281744881,\n",
       "  0.01565070629704826,\n",
       "  0.0156501943707437,\n",
       "  0.01564785514399689,\n",
       "  0.015644590311689726,\n",
       "  0.015662617436965044,\n",
       "  0.01563561590681512,\n",
       "  0.015644415980932046,\n",
       "  0.01562082181832399,\n",
       "  0.015627172982385026,\n",
       "  0.015616929914871406,\n",
       "  0.015608940618071382,\n",
       "  0.01561491434568273,\n",
       "  0.01561232107841078,\n",
       "  0.015654169463781377,\n",
       "  0.015583130543806797,\n",
       "  0.015590011253832458,\n",
       "  0.015585524049248215,\n",
       "  0.015573331186651001,\n",
       "  0.015574052744909852,\n",
       "  0.015634073257332147,\n",
       "  0.015553740903511306,\n",
       "  0.015558168227002147,\n",
       "  0.015563899255360114,\n",
       "  0.015549658209167977,\n",
       "  0.015545892147139058,\n",
       "  0.015566057861138087,\n",
       "  0.015537771497379985,\n",
       "  0.015561996944327182,\n",
       "  0.015532429445641063,\n",
       "  0.015528497242404848,\n",
       "  0.015537694315751168,\n",
       "  0.015535039167960901,\n",
       "  0.015515268812198458,\n",
       "  0.015511326898399838,\n",
       "  0.01552290951743561,\n",
       "  0.015504986531166266,\n",
       "  0.015556753195384757,\n",
       "  0.015528244078647363,\n",
       "  0.01549043899912034,\n",
       "  0.01548207717530631,\n",
       "  0.015503933034741945,\n",
       "  0.01554056897235204,\n",
       "  0.015492472847751928,\n",
       "  0.015471195558958225,\n",
       "  0.015467484270761557,\n",
       "  0.015480655449341177,\n",
       "  0.015458831613395046,\n",
       "  0.015499545264923467,\n",
       "  0.015449662036059214,\n",
       "  0.015466290476193143,\n",
       "  0.015457261457072968,\n",
       "  0.015460580263003834,\n",
       "  0.01543498687859622,\n",
       "  0.015444586204603126,\n",
       "  0.015476101552440915,\n",
       "  0.015451624707672005,\n",
       "  0.015433719178665399,\n",
       "  0.015423849538140034,\n",
       "  0.015423114549776675,\n",
       "  0.01542046747839708,\n",
       "  0.015422727169538436,\n",
       "  0.015407168829583717,\n",
       "  0.015415452109604459,\n",
       "  0.015397470524273657,\n",
       "  0.015430172136723968,\n",
       "  0.01540614260648759,\n",
       "  0.015412508038451274,\n",
       "  0.015399684946249012,\n",
       "  0.015403834608698125,\n",
       "  0.015393607958469975,\n",
       "  0.015377117901937227,\n",
       "  0.015384384094564765,\n",
       "  0.015369939113024209,\n",
       "  0.01539756815689079,\n",
       "  0.015363380677574296,\n",
       "  0.015385622008587493,\n",
       "  0.01536971430173893,\n",
       "  0.015362075978875237,\n",
       "  0.015358264102474288,\n",
       "  0.015343577544768552,\n",
       "  0.015361417339617144,\n",
       "  0.015356012883425548,\n",
       "  0.015379261537687709,\n",
       "  0.015346729264007777,\n",
       "  0.015343815463028935,\n",
       "  0.015342173921160076,\n",
       "  0.015325195729832222,\n",
       "  0.015365088165991966,\n",
       "  0.015341314725388371,\n",
       "  0.015325113962854952,\n",
       "  0.015323560281540888,\n",
       "  0.015326089931629414,\n",
       "  0.015336509154540922,\n",
       "  0.015307777070394601,\n",
       "  0.015311232860289162,\n",
       "  0.015311076569678918,\n",
       "  0.015306140830398135,\n",
       "  0.015312391969915145,\n",
       "  0.015324380826976633,\n",
       "  0.015287483003769798,\n",
       "  0.015326241056073634,\n",
       "  0.015293421743012594,\n",
       "  0.015315113650928136,\n",
       "  0.015327245685101465,\n",
       "  0.01530875584402337,\n",
       "  0.015274258011704978,\n",
       "  0.015296634453847307,\n",
       "  0.015279826839853045,\n",
       "  0.015270932639689065,\n",
       "  0.015302938283093303,\n",
       "  0.01527087059166469,\n",
       "  0.015268609845719235,\n",
       "  0.01527403623334613,\n",
       "  0.015319433049084856,\n",
       "  0.015272267356377248,\n",
       "  0.015306421157609948,\n",
       "  0.015256131280030184,\n",
       "  0.015257836845336993,\n",
       "  0.01526916739325808,\n",
       "  0.015247844590977182,\n",
       "  0.015250464522757289,\n",
       "  0.015245660801271668,\n",
       "  0.015240360407335896,\n",
       "  0.015240538479957872,\n",
       "  0.015235408586648735,\n",
       "  0.015277575481848422,\n",
       "  0.015239809221263775,\n",
       "  0.015227966948439664,\n",
       "  0.015267812973014093,\n",
       "  0.015244552273589682,\n",
       "  0.01523818899461044,\n",
       "  0.015248121074219228,\n",
       "  0.015280306804537494,\n",
       "  0.015222594526717028,\n",
       "  0.015217866025582378,\n",
       "  0.015213786632829185,\n",
       "  0.015237163091468746,\n",
       "  0.015217765209240102,\n",
       "  0.015221441295301449,\n",
       "  0.015213085834887763,\n",
       "  0.015237675847639031,\n",
       "  0.015198701201064077,\n",
       "  0.015232296907678083,\n",
       "  0.015203823744760512,\n",
       "  0.015299297564244927,\n",
       "  0.015195730785324182,\n",
       "  0.015200924829564252,\n",
       "  0.015194644430440885,\n",
       "  0.015217533566446917,\n",
       "  0.015207029884347856,\n",
       "  0.015213509946268257,\n",
       "  0.015226417374239496,\n",
       "  0.015186859165436767,\n",
       "  0.0151854795360767,\n",
       "  0.01518559378100666,\n",
       "  0.015217590267246504,\n",
       "  0.015227261281692965,\n",
       "  0.015249925036089611,\n",
       "  0.01524085418653843,\n",
       "  0.015176893492958922,\n",
       "  0.015197057345729013,\n",
       "  0.015176636333739849,\n",
       "  0.01517431045502035,\n",
       "  0.015171529926424022,\n",
       "  0.015170231706532353,\n",
       "  0.015188490528820361,\n",
       "  0.015174209883140286,\n",
       "  0.015182578146651277,\n",
       "  0.015168064716745172,\n",
       "  0.015207398116536031,\n",
       "  0.015197412393765352,\n",
       "  0.015197353557109597,\n",
       "  0.015185926222470222,\n",
       "  0.015167003562212452,\n",
       "  0.015187149488007288,\n",
       "  0.015164522038341786,\n",
       "  0.015165155088101356,\n",
       "  0.015158064849185854,\n",
       "  0.015155849253252482,\n",
       "  0.015159990990448088,\n",
       "  0.015149800305761647,\n",
       "  0.01516181659873487,\n",
       "  0.015157605584102524,\n",
       "  0.015191207343080599,\n",
       "  0.015138740846550926,\n",
       "  0.015136517421326082,\n",
       "  0.015161119089818462,\n",
       "  0.015159321586063385,\n",
       "  0.015206637241012606,\n",
       "  0.01514860464841905,\n",
       "  0.01513641125204083,\n",
       "  0.01513999908106142,\n",
       "  0.015155340771245394,\n",
       "  0.01517494975211819,\n",
       "  0.015130913965069905,\n",
       "  0.015142743951716981,\n",
       "  0.015124948497706106,\n",
       "  0.015135331774013855,\n",
       "  0.015128171776224716,\n",
       "  0.015143680697777362,\n",
       "  0.015132598471158398,\n",
       "  0.015139596777668143,\n",
       "  0.015127794303050548,\n",
       "  0.015121966383870653,\n",
       "  0.015171626155019805,\n",
       "  0.015151785382366846,\n",
       "  0.015134428211352257,\n",
       "  0.01513883072931506,\n",
       "  0.01511341639993075,\n",
       "  0.015111441997092414,\n",
       "  0.01510735362886787,\n",
       "  0.01512860136211103,\n",
       "  0.01510569030159714,\n",
       "  0.015144986772988126,\n",
       "  0.015110072858913184,\n",
       "  0.015113880373565928,\n",
       "  0.015166547644593976,\n",
       "  0.015124388787408755,\n",
       "  0.015108372020418747,\n",
       "  0.015137775735722237,\n",
       "  0.0151061516256816,\n",
       "  0.015101522986850888,\n",
       "  0.015093484260310734,\n",
       "  0.01514491452039272,\n",
       "  0.01509922941415766,\n",
       "  0.015104454418031646,\n",
       "  0.015115331130836783,\n",
       "  0.015113420839850298,\n",
       "  0.015091754562154225,\n",
       "  0.015125922401510186,\n",
       "  0.015093966636184298,\n",
       "  0.015102327284629732,\n",
       "  0.015131321828361159,\n",
       "  0.015082498342098877,\n",
       "  0.015118304595593572,\n",
       "  0.015112611868647765,\n",
       "  0.015109908713984195,\n",
       "  0.015077189770849904,\n",
       "  0.015085631788658056,\n",
       "  0.015092416737732072,\n",
       "  0.015103965519793741,\n",
       "  0.015085663800413584,\n",
       "  0.015087667165046023,\n",
       "  0.015078091544305509,\n",
       "  0.015119503067365687,\n",
       "  0.015079701364750037,\n",
       "  0.01507713559112464,\n",
       "  0.015086313467254985,\n",
       "  0.015088535845715614,\n",
       "  0.015071647039289168,\n",
       "  0.015082828329490776,\n",
       "  0.015080945425133342,\n",
       "  0.015080563165312687,\n",
       "  0.015097372345429053,\n",
       "  0.015068206994008607,\n",
       "  0.015084921049827544,\n",
       "  0.015071058889450133,\n",
       "  0.015067658634899865,\n",
       "  0.015062628070936033,\n",
       "  0.015088652040224535,\n",
       "  0.015121193759218643,\n",
       "  0.015074860495020282,\n",
       "  0.015057185539935458,\n",
       "  0.01506075131418784,\n",
       "  0.015089188825620698,\n",
       "  0.015060543989567216,\n",
       "  0.015061005789188025,\n",
       "  0.015081901231039196,\n",
       "  0.015060967000018324,\n",
       "  0.015058582670760486,\n",
       "  0.015078018247097055,\n",
       "  0.015052317661865886,\n",
       "  0.015083312602997544,\n",
       "  0.015071315715873697,\n",
       "  0.015051813200186826,\n",
       "  0.015083848197363362,\n",
       "  0.015048206305699377,\n",
       "  0.015059805918683153,\n",
       "  0.015094070737327742,\n",
       "  0.015046949859015831,\n",
       "  0.015044265090196804,\n",
       "  0.015053300318258406,\n",
       "  0.015056956093436035,\n",
       "  0.01506645134260067,\n",
       "  0.015035374380306403,\n",
       "  0.01506513501261061,\n",
       "  0.015049532696590219,\n",
       "  0.015068320498058365,\n",
       "  0.015052282781197976,\n",
       "  0.01505301463992342,\n",
       "  0.015102636887416003,\n",
       "  0.015097030840352776,\n",
       "  0.015081191417360446,\n",
       "  0.015031376835493323,\n",
       "  0.015037065567808703,\n",
       "  0.015050956896542058,\n",
       "  0.015035344674095555,\n",
       "  0.015030640454475556,\n",
       "  0.015037061562308305,\n",
       "  0.015045488320890369,\n",
       "  0.01503381408654558,\n",
       "  0.015033056634747568,\n",
       "  0.015025866166881103,\n",
       "  0.015029043523069874,\n",
       "  0.015030105905408464,\n",
       "  0.015023853876877918,\n",
       "  0.015055140036506095,\n",
       "  0.015031450389987983,\n",
       "  0.01503409507742456,\n",
       "  0.015121719097092135,\n",
       "  0.015027019333432997,\n",
       "  0.01501657304844442,\n",
       "  0.01502104218173365,\n",
       "  0.015054353924055292,\n",
       "  0.015020570831711998,\n",
       "  0.015034972768552554,\n",
       "  0.015021172852115492,\n",
       "  0.015032166333748594,\n",
       "  0.01504341807675462,\n",
       "  0.01502721423060845,\n",
       "  0.015018607612971639,\n",
       "  0.01502276295243606,\n",
       "  0.015057269798923353,\n",
       "  0.015038869466900863,\n",
       "  0.015018697850641416,\n",
       "  0.015063452490567295,\n",
       "  0.015027421444457192,\n",
       "  0.015012117303007813,\n",
       "  0.015017163875525655,\n",
       "  0.015010321086388794,\n",
       "  0.015014784413191146,\n",
       "  0.01501643438733874,\n",
       "  0.01507476597062446,\n",
       "  0.01504070260765084,\n",
       "  0.015055785458000194,\n",
       "  0.015013365575437082,\n",
       "  0.015011779817777722,\n",
       "  0.015028325672813841,\n",
       "  0.015001600116298404,\n",
       "  0.015015739692138484,\n",
       "  0.015018923915972367,\n",
       "  0.015004015617070219,\n",
       "  0.01500343458022157,\n",
       "  0.015012269050008032,\n",
       "  0.015016072854983813,\n",
       "  0.014995363609896992,\n",
       "  0.015004504651845814,\n",
       "  0.014996001533254213,\n",
       "  0.015014735484848057,\n",
       "  0.015016473404026233,\n",
       "  0.015056930593974855,\n",
       "  0.015000585243722098,\n",
       "  0.015036428071474317,\n",
       "  0.015004242971934463,\n",
       "  0.015006520874360661,\n",
       "  0.01500744625170002,\n",
       "  0.01501484980023814,\n",
       "  0.014997889175323187,\n",
       "  0.015014909323731675,\n",
       "  0.014994494625096635,\n",
       "  0.014996686368672744,\n",
       "  0.015022824223519223,\n",
       "  0.01504771787958047,\n",
       "  0.014984525392029768,\n",
       "  0.014991199066266293,\n",
       "  0.014992780374059957,\n",
       "  0.015001935535759545,\n",
       "  0.015019886588655438,\n",
       "  0.014994172703673866,\n",
       "  0.014987755837758568,\n",
       "  0.014999617414084222,\n",
       "  0.014991151924531024,\n",
       "  0.014988700682611567,\n",
       "  0.014983932054140768,\n",
       "  0.015018567767878552,\n",
       "  0.01502270476279545,\n",
       "  0.015023030009536132,\n",
       "  0.014985818578428357,\n",
       "  0.014982314310292117,\n",
       "  0.015015858334800097,\n",
       "  0.014978386661944539,\n",
       "  0.014980836407846023,\n",
       "  0.01500362271037105,\n",
       "  0.015001317966124417,\n",
       "  0.014980584270178099,\n",
       "  0.014978800484847694,\n",
       "  0.014980339553913227,\n",
       "  0.014993013616979583,\n",
       "  0.014997101326640254,\n",
       "  0.014989244905225413,\n",
       "  0.015081202146774497,\n",
       "  0.014982208309100015,\n",
       "  0.01502592143960407,\n",
       "  0.01498062710760575,\n",
       "  0.014976026379744255,\n",
       "  0.01497354457697714,\n",
       "  0.014982004673932767,\n",
       "  0.014986603498978636,\n",
       "  0.014972363029839647,\n",
       "  0.01500327182031034,\n",
       "  0.014972258446913491,\n",
       "  0.015005270809356468,\n",
       "  0.014992894578269661,\n",
       "  0.01497327413680537,\n",
       "  0.014984446791433194,\n",
       "  0.014972523655875605,\n",
       "  0.015001887961933733,\n",
       "  0.014978785616387479,\n",
       "  0.0149711688121479,\n",
       "  0.014999947261198552,\n",
       "  0.014961896130907872,\n",
       "  0.01501002121317871,\n",
       "  0.014986151960120911,\n",
       "  0.014964834484314623,\n",
       "  0.014992623716248491,\n",
       "  0.014983404057891614,\n",
       "  0.014959801537929002,\n",
       "  0.01498075982170151,\n",
       "  0.014982926895604287,\n",
       "  0.0149685246943743,\n",
       "  0.015001357621132864,\n",
       "  0.014995756844747654,\n",
       "  0.01497171669814776,\n",
       "  0.01497101992678133,\n",
       "  0.014973575494646563,\n",
       "  0.014960303286657234,\n",
       "  0.014966862278083843,\n",
       "  0.014978396668211828,\n",
       "  0.014959590334101299,\n",
       "  0.015009003089976759,\n",
       "  0.014956171035514871,\n",
       "  0.01499243075620207,\n",
       "  0.01499096076773108,\n",
       "  0.014955820472514017,\n",
       "  0.014970791929959038,\n",
       "  0.014959089271152278,\n",
       "  0.01496080798870429,\n",
       "  0.014961074289142945,\n",
       "  0.014967546919404091,\n",
       "  0.014963648424487084,\n",
       "  0.01497100088620996,\n",
       "  0.014960240800870316,\n",
       "  0.014949745501762817,\n",
       "  0.01497801529825217,\n",
       "  0.014999051292135215,\n",
       "  0.014954178747014163,\n",
       "  0.015008523618764915,\n",
       "  0.014964813366296415,\n",
       "  0.014964178421690153,\n",
       "  0.014960955314356308,\n",
       "  0.014971184404829922,\n",
       "  0.014968772890519857,\n",
       "  0.014975639238184879,\n",
       "  0.014975365338021619,\n",
       "  0.014980725047280579,\n",
       "  0.014989277807979281,\n",
       "  0.014970329110686024,\n",
       "  0.014954313590618846,\n",
       "  0.014944784064529816,\n",
       "  0.014946164874243982,\n",
       "  0.014968600095828205,\n",
       "  0.014978633710760369,\n",
       "  0.014944402433142266,\n",
       "  0.014949072202366145,\n",
       "  0.014982336337218198,\n",
       "  0.014956903687077977,\n",
       "  0.01495641654768263,\n",
       "  0.014981437485231333,\n",
       "  0.014977080386973271,\n",
       "  0.014964669754946687,\n",
       "  0.014957903872871523,\n",
       "  0.014944223058674629,\n",
       "  0.014939245525339215,\n",
       "  0.014942245461496325,\n",
       "  0.014982450279095911,\n",
       "  0.014955063401403693,\n",
       "  0.014956814518989189,\n",
       "  0.01494761319727933,\n",
       "  0.014942237703751637,\n",
       "  0.014941511345720573,\n",
       "  0.014940391702591316,\n",
       "  0.01494791588035104,\n",
       "  0.014946952642508084,\n",
       "  0.014955679751150438,\n",
       "  0.014946534768771974,\n",
       "  0.014963036954981357,\n",
       "  0.014943600417471437,\n",
       "  0.014938669380712786,\n",
       "  0.014966772827612173,\n",
       "  0.014963146917588183,\n",
       "  0.014971202558075379,\n",
       "  0.014962473763483749,\n",
       "  0.0149685651202983,\n",
       "  0.014990354635942477,\n",
       "  0.014938407888734215,\n",
       "  0.014933827719118288,\n",
       "  0.014936866840661117,\n",
       "  0.014935873637276819,\n",
       "  0.01495381899345108,\n",
       "  0.015006520829566232,\n",
       "  0.014934325422135592,\n",
       "  0.014948147298656709,\n",
       "  0.014975209686954028,\n",
       "  0.014938523486374835,\n",
       "  0.014928251993106652,\n",
       "  0.014929261439553513,\n",
       "  0.014940950903868929,\n",
       "  0.014950235619718347,\n",
       "  0.014980090227893076,\n",
       "  0.014961731799334977,\n",
       "  0.014930222063359882,\n",
       "  0.014938573200708674,\n",
       "  0.014934049785129317,\n",
       "  0.014932430782217038,\n",
       "  0.014928182393921035,\n",
       "  0.014942673544442281,\n",
       "  0.0149272104015058,\n",
       "  0.014947401667092764,\n",
       "  0.014951272787478681,\n",
       "  0.014971665650522738,\n",
       "  0.01494196604781518,\n",
       "  0.01494159731496134,\n",
       "  0.014924861846649004,\n",
       "  0.014978774856446886,\n",
       "  0.014930092716579472,\n",
       "  0.014924713165676647,\n",
       "  0.014929528793348098,\n",
       "  0.014924989118574913,\n",
       "  0.014936697199477902,\n",
       "  0.014951874846879947,\n",
       "  0.01493419080108272,\n",
       "  0.014946843959937292,\n",
       "  0.014961967702683671,\n",
       "  0.014928964235649033,\n",
       "  0.01494020823718528,\n",
       "  0.014921333824958388,\n",
       "  0.014931059880148114,\n",
       "  0.01492134266288259,\n",
       "  0.014929825760480677,\n",
       "  0.01494439600192787,\n",
       "  0.014936640325781415,\n",
       "  0.0149375857011848,\n",
       "  0.014959336870708592,\n",
       "  0.0149565343152547,\n",
       "  0.014927550599458298,\n",
       "  0.014924049439402986,\n",
       "  0.014992361017872224,\n",
       "  0.014943696527209038,\n",
       "  0.014944260855071528,\n",
       "  0.01493701661485633,\n",
       "  0.01493506532740643,\n",
       "  0.014919475121017314,\n",
       "  0.014925097063965215,\n",
       "  0.014930698895998702,\n",
       "  0.014954128119518292,\n",
       "  0.014919956611095958,\n",
       "  0.014934259928791055,\n",
       "  0.01492864593216631,\n",
       "  0.014949416155532635,\n",
       "  0.014926550200877488,\n",
       "  0.014974672007437942,\n",
       "  0.014929154594406299,\n",
       "  0.014920548824558438,\n",
       "  0.014923660074682835,\n",
       "  0.015006943529088617,\n",
       "  0.014925642000660896,\n",
       "  0.014942998612915462,\n",
       "  0.014913727159932725,\n",
       "  0.014934494094385038,\n",
       "  0.014922850461458628,\n",
       "  0.01494235381999881,\n",
       "  0.014925326566935222,\n",
       "  0.014923522993799019,\n",
       "  0.014936076315763007,\n",
       "  0.0149137422032159,\n",
       "  0.014939895184917753,\n",
       "  0.014941057579052671,\n",
       "  0.014914395215135892,\n",
       "  0.014915477918651826,\n",
       "  0.014937406839602303,\n",
       "  0.014911044132456601,\n",
       "  0.014921572680379734,\n",
       "  0.01491581355405939,\n",
       "  0.014927504473379307,\n",
       "  0.014926396054700027,\n",
       "  0.014942961206133134,\n",
       "  0.014924622667602567,\n",
       "  0.014936268053986138,\n",
       "  0.014918781042869743,\n",
       "  0.014917133793814244,\n",
       "  0.01493574339176427,\n",
       "  0.01490779646573069,\n",
       "  0.014913573879764748,\n",
       "  0.01491683121529813,\n",
       "  0.014914833248458338,\n",
       "  0.014929938410231174,\n",
       "  0.014912754238143505,\n",
       "  0.014981075951026946,\n",
       "  0.014911845597756648,\n",
       "  0.014911071531507291,\n",
       "  0.014911711117992539,\n",
       "  0.014920144947262943,\n",
       "  0.014905284404829933,\n",
       "  0.014943471135284726,\n",
       "  0.014909603529121972,\n",
       "  0.014919579842835332,\n",
       "  0.014922413818027709,\n",
       "  0.01490304729109237,\n",
       "  0.014905246200239227,\n",
       "  0.014935115845371561,\n",
       "  0.014929736141077505,\n",
       "  0.014904777391985622,\n",
       "  0.014922177806860568,\n",
       "  0.01490286884779285,\n",
       "  0.014919386851328775,\n",
       "  0.01490489168860723,\n",
       "  0.014936427211398984,\n",
       "  0.014930611981807633,\n",
       "  0.014908045047459916,\n",
       "  0.014900075487160899,\n",
       "  0.01490858553762501,\n",
       "  0.014909132339672242,\n",
       "  0.014907029817257474,\n",
       "  0.01492254694806198,\n",
       "  0.014915577246910405,\n",
       "  0.014907585382190527,\n",
       "  0.014910169292095623,\n",
       "  0.014929803969546218,\n",
       "  0.014914845215126078,\n",
       "  0.014903748153124325,\n",
       "  0.014905854528716114,\n",
       "  0.014925138745345812,\n",
       "  0.014907347388018974,\n",
       "  0.014909565162152473,\n",
       "  0.014902753670905693,\n",
       "  0.01489843107361937,\n",
       "  0.014903705952686289,\n",
       "  0.014924799198340128,\n",
       "  0.014919621040879535,\n",
       "  0.014971173484810339,\n",
       "  ...]}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=234)\n",
    "\n",
    "best = {}\n",
    "best['score'] = 0\n",
    "for c in [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]:\n",
    "    m = SVM(C=c).fit(X_train, y_train)\n",
    "    s = m.score(X_test, y_test)\n",
    "    print(f\"current score is {s} at C={c}\")\n",
    "    if s > best['score']:\n",
    "        print(f\"current best score is {s} at C={c}\")\n",
    "        best['m'] = m\n",
    "        best['score'] = s\n",
    "        best['C'] = c\n",
    "        best['losses'] = m.losses\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel trick을 이용한 SVM\n",
    "\n",
    "지금까지 선형 SVM에 대해 설명했는데, 선형 SVM으로는 데이터를 제대로 분류할 수 없는 상황들이 상당히 많다. 그런 경우를 해결하기 위해 방법 중에서 위에서 설명한 C-SVM과 함께 가장 가장 널리 활용되고 있는 것은 바로 커널 트릭을 사용한 SVM이다. \n",
    "![](https://t1.daumcdn.net/cfile/tistory/99F2D73359EACDE930)\n",
    "\n",
    "커널 트릭을 사용하는 방법 중에서 가장 많이 사용되는 방법이 RBF 커널로 고차원공간에서 데이터셋의 유사도(거리)측정하는 경우, 아래와 같은 features space를 생성하게 된다.\n",
    "![](https://t1.daumcdn.net/cfile/tistory/990CD33359E9EC961E)\n",
    "\n",
    "2차원 공간에서는 도저히 선형적으로 분류할 수 없을 것 같았던 데이터 분포가 커널 기법을 사용해서 3차원 features space에서 보면 분류가 가능해졌다. 3차원 공간에서 분류된 것을 다시 2차원 공간으로 매핑해서 보면 아래 그림과 같이 결정 경계가 둥그렇게 보인다.\n",
    "![](https://t1.daumcdn.net/cfile/tistory/99A1933359EACD3732)\n",
    "\n",
    "따라서 Kernelized SVM의 핵심 아이디어는 다음과 같다:\n",
    "> 원공간(Input Space)의 데이터를 선형분류가 가능한 고차원 공간(Feature Space)으로 매핑한 뒤 두 범주를 분류하는 초평면을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF 커널\n",
    "커널에는 Polynomial 커널, Sigmoid 커널, 가우시안 RBF 커널 등 종류가 많은데, 그 중 가장 성능이 좋아 자주 사용되는 것이 가우시안 RBF 커널이다. \n",
    "$$\\kappa(x_1, x_2) = \\exp \\left( -\\gamma ||x_1-x_2||^2 \\right)$$\n",
    "\n",
    "각 커널마다 최적화를 도와주는 매개변수들이 따로 있다. RBF 커널의 경우 gamma($\\gamma$)라는 매개변수를 사용자가 조정해야 한다. Kernelized C-SVM을 사용하는 경우, 기본 매개변수인 C도 있으므로 총 2개의 매개변수를 설정해줘야한다. 그렇다면 gamma의 역할은 무엇일까? gamma는 하나의 데이터 샘플이 영향력을 행사하는 거리를 결정한다. gamma는 가우시안 함수의 분산의 역수에 비례하므로, gamma가 클수록 작은 표준편차를 갖는다. \n",
    "\n",
    "즉, gamma가 크면:\n",
    "- 가우시안 커널의 분산 값이 작고\n",
    "- 이때, 두 점간의 거리가 조금만 멀어져도 커널의 값은 0에 급격히 가까워지므로\n",
    "- 결국 데이터 포인트들이 영향력을 미치는 거리가 짧아지게 된다.\n",
    "\n",
    "gamma가 작으면, 분산이 크고, 이 경우 두 데이터 포인터간의 거리가 상당히 떨어지더라도 커널 값은 쉽게 0으로 줄어들 지 않으므로, 각 데이터 포인트들이 영향을 미치는 거리가 길어진다.\n",
    "\n",
    "gamma의 크기에 따른 가우시안 함수의 모양은 아래와 같다.\n",
    "![](https://i.imgur.com/AxsNDpu.png)\n",
    "\n",
    "gamma와 C의 영향도를 확인하기 위해 아래 그림을 살펴보자. 오른쪽으로 갈수록 gamma가 커지고, 아래 쪽으로 내려갈수록 C가 커진다.\n",
    "![](https://t1.daumcdn.net/cfile/tistory/996CB13359EB266A0F)\n",
    "1. C가 커질수록, 분류할 수 없는 이상치를 더 줄이고자 하고, \n",
    " - 이로부터 마진은 작아지면서, overfit이 발생한다.\n",
    "2. gamma가 커질수록, 가우시안 커널의 분산이 작아져서\n",
    " - 두 데이터가 조금만 떨어져도 유사도가 급격히 작아지게 되고\n",
    " - 이로 인해, 결정경계는 결정경계 가까이에 있는 데이터에 의해서만 영향을 받게되어\n",
    " - 결정경계가 매우 구불구불해진다.\n",
    "3. C도 크고, gamma도 큰 경우에는\n",
    " - 마직이 줄어들고, 곡률도 커져서 결국 결정경계가 나누어지는 극적인 효과가 나타난다.\n",
    " - 이런 경우 훈련데이터에 과도하게 overfit이 발생하므로 매우 나쁜 모델이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 커널 트릭\n",
    "그런데 고차원공간으로의 매핑함수를 도입하게 되면 연산량이 폭증하게 된다. \n",
    "1. 모든 관측치에 대해 고차원으로 매핑하고,\n",
    "2. 이를 다시 내적(inner product)해야 한다. \n",
    "\n",
    "고차원 매핑과 내적을 한방에 해결하기 위해 도입된 것이 바로 커널(Kernel)이다. 커널 K는 다음과 같이 정의된다.\n",
    "$$\\kappa(\\mathbf x_i, \\mathbf x_j) = \\Phi(\\mathbf x_i)^T \\Phi(\\mathbf x_j) \\quad \\cdots \\quad (21)$$\n",
    "\n",
    "위와 같은 내적의 특성을 갖기 때문에 kernel은 다음과 같은 성질을 갖는다:\n",
    "1. $\\kappa(\\mathbf x_i, \\mathbf x_j) = \\kappa(\\mathbf x_j, \\mathbf x_i) \\ge 0$\n",
    "2. $\\kappa(\\mathbf x_i, \\mathbf x_j)$로 구성된 행렬은 positive semi-definite matrix.\n",
    "3. 1과 같은 성질로 부터 2의 matrix는 symmetric하다.\n",
    "4. $\\kappa(\\mathbf x_i, \\mathbf x_j) = \\sum_i \\lambda_i \\phi_i(\\mathbf x_i) \\phi_j(\\mathbf x_j)$로 분해할 수 있다.\n",
    "\n",
    "위와 같은 성질은 만족하는 커널을 Mercer 커널이라 하며, Mercer 커널은 모두 SVM의 커널함수로 사용할 수 있다.\n",
    "\n",
    "여러가지 커널함수:\n",
    "$$\\begin{align*}\n",
    "linear\\quad &:\\quad K({ \\mathbf x }_{ 1 },{ \\mathbf x }_{ 2 }) &&= { \\mathbf x }_{ 1 }^{ T }{ \\mathbf x }_{ 2 }, &\\\\\n",
    "polynomial\\quad &:\\quad K({ \\mathbf x }_{ 1 },{ \\mathbf x }_{ 2 }) &&= { (\\gamma({ \\mathbf x }_{ 1 }^{ T }{ \\mathbf x }_{ 2 })+r) }^{ d }, & r>0\\\\ \n",
    "sigmoid\\quad &:\\quad K({ \\mathbf x }_{ 1 },{ \\mathbf x }_{ 2 }) &&= \\tanh { \\left\\{ \\gamma ({ \\mathbf x }_{ 1 }^{ T }{ \\mathbf x }_{ 2 })+r \\right\\}  }, & \\gamma,r\\ge 0\\\\ \n",
    "gaussian\\quad &:\\quad K({ \\mathbf x }_{ 1 },{ \\mathbf x }_{ 2 }) &&= \\exp\\left\\{ -\\frac { { \\left\\| { \\mathbf x }_{ 1 }-{ \\mathbf x }_{ 2 } \\right\\|  }_{ 2 }^{ 2 } }{ 2{ \\sigma  }^{ 2 } }  \\right\\}, & \\sigma \\neq 0 \\\\\n",
    "&&&= \\exp\\left\\{ -\\gamma { \\left\\| { \\mathbf x }_{ 1 }-{ \\mathbf x }_{ 2 } \\right\\|  }_{ 2 }^{ 2 }  \\right\\}, & \\gamma = \\cfrac 1 {2 \\sigma^2}\n",
    "\\end{align*}$$\n",
    "\n",
    "이러한 커널함수를 사용할 때, 식(14)와 (20)은 다음과 같이 계산할 수 있다:\n",
    "$$\\max { { L }_{ D }({ \\alpha  }_{ i }) } =\\sum _{ i=1 }^{ n }{ { \\alpha  }_{ i } } -\\frac { 1 }{ 2 } \\sum _{ i=1 }^{ n }{ \\sum _{ j=1 }^{ n }{ { \\alpha  }_{ i }{ { \\alpha  }_{ j }y }_{ i }{ y }_{ j } K(\\mathbf x_i, \\mathbf x_j) }  } \\quad \\cdots \\quad (22)$$\n",
    "\n",
    "이와 같이 kernel 함수를 직접 사용하면: \n",
    "1. 매핑함수로 새로운 features를 생성하고, 새로운 features 간의 내적 연산을 할 필요 없이, 바로 목적함수를 계산할 수 있다.\n",
    "2. 다만, 고차원 공간에서의 클래스 분포를 알 수 없으므로, $\\alpha_i$에 대한 무작위 초기화를 할 필요성이 있는 지에 대해 의구심이 있을 수 있으나,\n",
    "3. 고차원에서의 결정경계도 저차원으로 사상될 때 linear SVM의 hyperplane 근방에서 곡선을 그릴 것이므로, 저자원에서 사용한 초기화 값을 유지해도 상관없다.\n",
    "\n",
    "##### prediction: decision function\n",
    "커널을 사용할 경우 새로운 데이터에 대한 prediction을 위한 decision function은 다음과 같다.\n",
    "$$\\text{sign} \\left( \\sum_{i=1}^n y_i \\alpha_i K(\\mathbf x_i, \\mathbf x) + b \\right) \\quad \\cdots \\quad (23)$$\n",
    "- decision_function 메소드의 인수로서 새로운 데이터 $\\mathbf x$를 전달한다.\n",
    "- rest points에서는 $\\alpha_j = 0$이므로 supports vector에 대해서만 계산하다.\n",
    "- 커널을 사용할 경우, 직접 $\\mathbf w$을 계산하기 어렵기 때문에 `svc.dual_coef_`에서 support vectors와 slack points(sklearn에서는 이것도 support vectors로 다룬다.)에 대한 $y_i \\alpha_i$를 제공한다.\n",
    "\n",
    "##### 커널의 효과\n",
    "기존 SVM에 해당하는 Linear 커널은 선형 분류경계면을 만들어내지만, 커널을 사용하면 비선형 경계면이 만들어진다.\n",
    "\n",
    "polynormial 커널을 사용한 경우:\n",
    "![](http://i.imgur.com/uVpdEym.png)\n",
    "\n",
    "가우시안 RBF 커널을 사용한 경우, gamma가 커질 수록 경계면의 곡률이 커지는 것을 확인할 수 있다:\n",
    "![](http://i.imgur.com/RciJQCj.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=iris.target)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_scaled, X_ts_scaled, X_scaled = scaler.transform(X_train), scaler.transform(X_test), scaler.transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5899979745524563e-15, 1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_scaled.mean(), X_tr_scaled.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = LinearSVC().fit(X_tr_scaled, y_train)\n",
    "score = lsvc.score(X_ts_scaled, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Quiz] C를 튜닝하여 성능의 변화를 관찰하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.7368421052631579,\n",
       " 0.001: 0.7631578947368421,\n",
       " 0.01: 0.7631578947368421,\n",
       " 0.1: 0.8947368421052632,\n",
       " 1: 0.9736842105263158,\n",
       " 10: 0.9736842105263158,\n",
       " 100: 0.9736842105263158,\n",
       " 1000: 0.9736842105263158,\n",
       " 10000: 0.9736842105263158}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {}\n",
    "for c in [.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]:\n",
    "    m = LinearSVC(C=c, max_iter=50000).fit(X_tr_scaled, y_train)\n",
    "    scores[c] = m.score(X_ts_scaled, y_test)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.3157894736842105,\n",
       " 0.001: 0.3157894736842105,\n",
       " 0.01: 0.3157894736842105,\n",
       " 0.1: 0.8157894736842105,\n",
       " 1: 0.9736842105263158,\n",
       " 10: 1.0,\n",
       " 100: 0.9736842105263158,\n",
       " 1000: 0.9473684210526315,\n",
       " 10000: 0.9473684210526315}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {}\n",
    "for c in [.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]:\n",
    "    m = SVC(C=c).fit(X_tr_scaled, y_train)\n",
    "    scores[c] = m.score(X_ts_scaled, y_test)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best': [(100, 0.1), (10000, 0.01)],\n",
       " 0.0001: {0.0001: 0.3157894736842105,\n",
       "  0.001: 0.3157894736842105,\n",
       "  0.01: 0.3157894736842105,\n",
       "  0.1: 0.3157894736842105,\n",
       "  1: 0.3157894736842105,\n",
       "  10: 0.3157894736842105,\n",
       "  100: 0.3157894736842105,\n",
       "  1000: 0.3157894736842105,\n",
       "  10000: 0.3157894736842105},\n",
       " 0.001: {0.0001: 0.3157894736842105,\n",
       "  0.001: 0.3157894736842105,\n",
       "  0.01: 0.3157894736842105,\n",
       "  0.1: 0.3157894736842105,\n",
       "  1: 0.3157894736842105,\n",
       "  10: 0.3157894736842105,\n",
       "  100: 0.3157894736842105,\n",
       "  1000: 0.3157894736842105,\n",
       "  10000: 0.3157894736842105},\n",
       " 0.01: {0.0001: 0.3157894736842105,\n",
       "  0.001: 0.3157894736842105,\n",
       "  0.01: 0.3157894736842105,\n",
       "  0.1: 0.3157894736842105,\n",
       "  1: 0.3157894736842105,\n",
       "  10: 0.3157894736842105,\n",
       "  100: 0.3157894736842105,\n",
       "  1000: 0.3157894736842105,\n",
       "  10000: 0.3157894736842105},\n",
       " 0.1: {0.0001: 0.3157894736842105,\n",
       "  0.001: 0.3157894736842105,\n",
       "  0.01: 0.5,\n",
       "  0.1: 0.8157894736842105,\n",
       "  1: 0.9473684210526315,\n",
       "  10: 0.3157894736842105,\n",
       "  100: 0.3157894736842105,\n",
       "  1000: 0.3157894736842105,\n",
       "  10000: 0.3157894736842105},\n",
       " 1: {0.0001: 0.3157894736842105,\n",
       "  0.001: 0.5,\n",
       "  0.01: 0.8157894736842105,\n",
       "  0.1: 0.9736842105263158,\n",
       "  1: 0.9736842105263158,\n",
       "  10: 0.8947368421052632,\n",
       "  100: 0.5789473684210527,\n",
       "  1000: 0.34210526315789475,\n",
       "  10000: 0.34210526315789475},\n",
       " 10: {0.0001: 0.5,\n",
       "  0.001: 0.8421052631578947,\n",
       "  0.01: 0.9736842105263158,\n",
       "  0.1: 0.9736842105263158,\n",
       "  1: 0.9473684210526315,\n",
       "  10: 0.8947368421052632,\n",
       "  100: 0.5789473684210527,\n",
       "  1000: 0.34210526315789475,\n",
       "  10000: 0.34210526315789475},\n",
       " 100: {0.0001: 0.8421052631578947,\n",
       "  0.001: 0.9473684210526315,\n",
       "  0.01: 0.9736842105263158,\n",
       "  0.1: 1.0,\n",
       "  1: 0.9473684210526315,\n",
       "  10: 0.8947368421052632,\n",
       "  100: 0.5789473684210527,\n",
       "  1000: 0.34210526315789475,\n",
       "  10000: 0.34210526315789475},\n",
       " 1000: {0.0001: 0.9473684210526315,\n",
       "  0.001: 0.9736842105263158,\n",
       "  0.01: 0.9736842105263158,\n",
       "  0.1: 0.9736842105263158,\n",
       "  1: 0.9473684210526315,\n",
       "  10: 0.8947368421052632,\n",
       "  100: 0.5789473684210527,\n",
       "  1000: 0.34210526315789475,\n",
       "  10000: 0.34210526315789475},\n",
       " 10000: {0.0001: 0.9736842105263158,\n",
       "  0.001: 0.9736842105263158,\n",
       "  0.01: 1.0,\n",
       "  0.1: 0.9473684210526315,\n",
       "  1: 0.9473684210526315,\n",
       "  10: 0.8947368421052632,\n",
       "  100: 0.5789473684210527,\n",
       "  1000: 0.34210526315789475,\n",
       "  10000: 0.34210526315789475}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {}\n",
    "scores['best'] = []\n",
    "for c in [.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]:\n",
    "    scores[c] = {}\n",
    "    for g in [.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]:\n",
    "        m = SVC(C=c, gamma=g).fit(X_tr_scaled, y_train)\n",
    "        score = m.score(X_ts_scaled, y_test)\n",
    "        scores[c][g] = score\n",
    "        if score == 1:\n",
    "            scores['best'].append((c, g)) \n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
